{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d25b9387",
   "metadata": {},
   "source": [
    "1. Dataset creation and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f73b6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b2441",
   "metadata": {},
   "source": [
    "## 1. Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d7de1376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "sentiment_labeled_data: (5791, 2)\n",
      "stock_tweets: (80793, 4)\n",
      "stock_prices: (6300, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "sentiment_labeled_data = pd.read_csv('data/raw/stock_data.csv')\n",
    "stock_tweets = pd.read_csv('data/raw/stock_tweets_d1.csv')\n",
    "stock_prices = pd.read_csv('data/raw/stock_yfinance_data_d1.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"sentiment_labeled_data: {sentiment_labeled_data.shape}\")\n",
    "print(f\"stock_tweets: {stock_tweets.shape}\")\n",
    "print(f\"stock_prices: {stock_prices.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f14838",
   "metadata": {},
   "source": [
    "## 2. Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c4faae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows:\n",
      "                                                Text  Sentiment\n",
      "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
      "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
      "2  user I'd be afraid to short AMZN - they are lo...          1\n",
      "3                                  MNTA Over 12.00            1\n",
      "4                                   OI  Over 21.37            1\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5791 entries, 0 to 5790\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Text       5791 non-null   object\n",
      " 1   Sentiment  5791 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 90.6+ KB\n",
      "None\n",
      "\n",
      "Sentiment distribution:\n",
      "Sentiment\n",
      " 1    3685\n",
      "-1    2106\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Text         0\n",
      "Sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore sentiment_labeled_data (labeled sentiment data)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(sentiment_labeled_data.head())\n",
    "print(\"\\nInfo:\")\n",
    "print(sentiment_labeled_data.info())\n",
    "print(\"\\nSentiment distribution:\")\n",
    "print(sentiment_labeled_data['Sentiment'].value_counts())\n",
    "print(\"\\nMissing values:\")\n",
    "print(sentiment_labeled_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eaec6b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows:\n",
      "                        Date  \\\n",
      "0  2022-09-29 23:41:16+00:00   \n",
      "1  2022-09-29 23:24:43+00:00   \n",
      "2  2022-09-29 23:18:08+00:00   \n",
      "3  2022-09-29 22:40:07+00:00   \n",
      "4  2022-09-29 22:27:05+00:00   \n",
      "\n",
      "                                               Tweet Stock Name Company Name  \n",
      "0  Mainstream media has done an amazing job at br...       TSLA  Tesla, Inc.  \n",
      "1  Tesla delivery estimates are at around 364k fr...       TSLA  Tesla, Inc.  \n",
      "2  3/ Even if I include 63.0M unvested RSUs as of...       TSLA  Tesla, Inc.  \n",
      "3  @RealDanODowd @WholeMarsBlog @Tesla Hahaha why...       TSLA  Tesla, Inc.  \n",
      "4  @RealDanODowd @Tesla Stop trying to kill kids,...       TSLA  Tesla, Inc.  \n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80793 entries, 0 to 80792\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Date          80793 non-null  object\n",
      " 1   Tweet         80793 non-null  object\n",
      " 2   Stock Name    80793 non-null  object\n",
      " 3   Company Name  80793 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 2.5+ MB\n",
      "None\n",
      "\n",
      "Unique stocks:\n",
      "Stock Name\n",
      "TSLA    37422\n",
      "TSM     11034\n",
      "AAPL     5056\n",
      "PG       4089\n",
      "AMZN     4089\n",
      "MSFT     4089\n",
      "NIO      3021\n",
      "META     2751\n",
      "AMD      2227\n",
      "NFLX     1727\n",
      "GOOG     1291\n",
      "PYPL      843\n",
      "DIS       635\n",
      "BA        399\n",
      "COST      393\n",
      "INTC      315\n",
      "KO        310\n",
      "CRM       233\n",
      "XPEV      225\n",
      "ENPH      216\n",
      "ZS        193\n",
      "VZ        123\n",
      "BX         50\n",
      "NOC        31\n",
      "F          31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Date            0\n",
      "Tweet           0\n",
      "Stock Name      0\n",
      "Company Name    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore stock_tweets (tweets with stock info)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(stock_tweets.head())\n",
    "print(\"\\nInfo:\")\n",
    "print(stock_tweets.info())\n",
    "print(\"\\nUnique stocks:\")\n",
    "print(stock_tweets['Stock Name'].value_counts())\n",
    "print(\"\\nMissing values:\")\n",
    "print(stock_tweets.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b74b5b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows:\n",
      "         Date        Open        High         Low       Close   Adj Close  \\\n",
      "0  2021-09-30  260.333344  263.043335  258.333344  258.493347  258.493347   \n",
      "1  2021-10-01  259.466675  260.260010  254.529999  258.406677  258.406677   \n",
      "2  2021-10-04  265.500000  268.989990  258.706665  260.510010  260.510010   \n",
      "3  2021-10-05  261.600006  265.769989  258.066681  260.196655  260.196655   \n",
      "4  2021-10-06  258.733337  262.220001  257.739990  260.916656  260.916656   \n",
      "\n",
      "     Volume Stock Name  \n",
      "0  53868000       TSLA  \n",
      "1  51094200       TSLA  \n",
      "2  91449900       TSLA  \n",
      "3  55297800       TSLA  \n",
      "4  43898400       TSLA  \n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6300 entries, 0 to 6299\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Date        6300 non-null   object \n",
      " 1   Open        6300 non-null   float64\n",
      " 2   High        6300 non-null   float64\n",
      " 3   Low         6300 non-null   float64\n",
      " 4   Close       6300 non-null   float64\n",
      " 5   Adj Close   6300 non-null   float64\n",
      " 6   Volume      6300 non-null   int64  \n",
      " 7   Stock Name  6300 non-null   object \n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 393.9+ KB\n",
      "None\n",
      "\n",
      "Date range:\n",
      "From 2021-09-30 00:00:00 to 2022-09-29 00:00:00\n",
      "\n",
      "Unique stocks:\n",
      "Stock Name\n",
      "TSLA    252\n",
      "DIS     252\n",
      "ZS      252\n",
      "NIO     252\n",
      "ENPH    252\n",
      "PYPL    252\n",
      "NOC     252\n",
      "BX      252\n",
      "BA      252\n",
      "INTC    252\n",
      "CRM     252\n",
      "VZ      252\n",
      "COST    252\n",
      "MSFT    252\n",
      "F       252\n",
      "KO      252\n",
      "TSM     252\n",
      "NFLX    252\n",
      "AAPL    252\n",
      "AMD     252\n",
      "GOOG    252\n",
      "AMZN    252\n",
      "META    252\n",
      "PG      252\n",
      "XPEV    252\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Date          0\n",
      "Open          0\n",
      "High          0\n",
      "Low           0\n",
      "Close         0\n",
      "Adj Close     0\n",
      "Volume        0\n",
      "Stock Name    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Explore stock_prices (yfinance data)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(stock_prices.head())\n",
    "print(\"\\nInfo:\")\n",
    "print(stock_prices.info())\n",
    "print(\"\\nDate range:\")\n",
    "stock_prices['Date'] = pd.to_datetime(stock_prices['Date'])\n",
    "print(f\"From {stock_prices['Date'].min()} to {stock_prices['Date'].max()}\")\n",
    "print(\"\\nUnique stocks:\")\n",
    "print(stock_prices['Stock Name'].value_counts())\n",
    "print(\"\\nMissing values:\")\n",
    "print(stock_prices.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33830898",
   "metadata": {},
   "source": [
    "## 3. Clean and Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function with ticker preservation\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess tweet text, preserves stock ticker symbols as ticker_XXX\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \n",
    "    \n",
    "    # Extract tickers (case-insensitive, 1-5 characters) and replace with ticker_XXX format\n",
    "    # Use case-insensitive matching to catch $tsla, $TSLA, $TsLa, etc.\n",
    "    tickers = re.findall(r'\\$([A-Za-z]{1,5})\\b', text)\n",
    "    \n",
    "    # Replace each ticker with ticker_UPPERCASE format (remove the $)\n",
    "    for ticker in tickers:\n",
    "        # Replace both $TICKER and any case variations\n",
    "        text = re.sub(rf'\\${ticker}\\b', f'ticker_{ticker.upper()}', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Convert to lowercase (tickers are already preserved as ticker_XXX)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove user mentions and hashtags symbols (but keep the text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove HTML entities like &amp;\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    \n",
    "    # Keep letters, digits, spaces, underscores (for ticker_xxx), and financial symbols\n",
    "    text = re.sub(r'[^a-z0-9\\s%._]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9459f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing clean_text function ===\n",
      "\n",
      "Test 1:\n",
      "Original: Tesla delivery estimates are at around 364k from the analysts. $tsla\n",
      "Cleaned:  tesla delivery estimates are at around 364k from the analysts. tsla\n",
      "\n",
      "Test 2:\n",
      "Original: $NIO just because I'm down money doesn't mean this is a bad investment. $AAPL $AMZN $TSLA $GOOGL $NIO\n",
      "Cleaned:  nio just because i m down money doesn t mean this is a bad investment. aapl amzn tsla googl nio\n",
      "\n",
      "Test 3:\n",
      "Original: 3/ Even if I include 63.0M unvested RSUs as of 6/30, additional equity needed for the RSUs is 63.0M x $54.20 = $3.4B. $twtr $tsla\n",
      "Cleaned:  3 even if i include 63.0m unvested rsus as of 6 30 additional equity needed for the rsus is 63.0m x 54.20 3.4b. twtr tsla\n",
      "\n",
      "Test 4:\n",
      "Original: Mainstream media has done an amazing job at brainwashing people. @Tesla &amp; EVERYONE disagreed\n",
      "Cleaned:  mainstream media has done an amazing job at brainwashing people. everyone disagreed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the improved clean_text function\n",
    "test_tweets = [\n",
    "    \"Tesla delivery estimates are at around 364k from the analysts. $tsla\",  # lowercase ticker\n",
    "    \"$NIO just because I'm down money doesn't mean this is a bad investment. $AAPL $AMZN $TSLA $GOOGL $NIO\",  # multiple tickers\n",
    "    \"3/ Even if I include 63.0M unvested RSUs as of 6/30, additional equity needed for the RSUs is 63.0M x $54.20 = $3.4B. $twtr $tsla\",  # mixed case\n",
    "    \"Mainstream media has done an amazing job at brainwashing people. @Tesla &amp; EVERYONE disagreed\",  # HTML entity\n",
    "]\n",
    "\n",
    "print(\"=== Testing clean_text function ===\\n\")\n",
    "for i, tweet in enumerate(test_tweets, 1):\n",
    "    cleaned = clean_text(tweet)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Original: {tweet}\")\n",
    "    print(f\"Cleaned:  {cleaned}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d3f960f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 5791\n",
      "Rows after cleaning: 5791\n",
      "\n",
      "Sample cleaned tweets:\n",
      "                                                Text  \\\n",
      "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...   \n",
      "1  user: AAP MOVIE. 55% return for the FEA/GEED i...   \n",
      "2  user I'd be afraid to short AMZN - they are lo...   \n",
      "3                                  MNTA Over 12.00     \n",
      "4                                   OI  Over 21.37     \n",
      "\n",
      "                                        cleaned_text  Sentiment  \n",
      "0  kickers on my watchlist xide tit soq pnk cpw b...          1  \n",
      "1  user aap movie. 55% return for the fea geed in...          1  \n",
      "2  user i d be afraid to short amzn they are look...          1  \n",
      "3                                    mnta over 12.00          1  \n",
      "4                                      oi over 21.37          1  \n"
     ]
    }
   ],
   "source": [
    "# Process sentiment_labeled_data (labeled sentiment tweets)\n",
    "sentiment_labeled_data_cleaned = sentiment_labeled_data.copy()\n",
    "\n",
    "# Clean the text\n",
    "sentiment_labeled_data_cleaned['cleaned_text'] = sentiment_labeled_data_cleaned['Text'].apply(clean_text)\n",
    "\n",
    "# Remove rows with empty cleaned text\n",
    "sentiment_labeled_data_cleaned = sentiment_labeled_data_cleaned[sentiment_labeled_data_cleaned['cleaned_text'].str.len() > 0]\n",
    "\n",
    "print(f\"Original rows: {len(sentiment_labeled_data)}\")\n",
    "print(f\"Rows after cleaning: {len(sentiment_labeled_data_cleaned)}\")\n",
    "print(f\"\\nSample cleaned tweets:\")\n",
    "print(sentiment_labeled_data_cleaned[['Text', 'cleaned_text', 'Sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d6f0c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 80793\n",
      "Rows after cleaning: 80792\n",
      "\n",
      "Sample cleaned tweets:\n",
      "                       Date  \\\n",
      "0 2022-09-29 23:41:16+00:00   \n",
      "1 2022-09-29 23:24:43+00:00   \n",
      "2 2022-09-29 23:18:08+00:00   \n",
      "3 2022-09-29 22:40:07+00:00   \n",
      "4 2022-09-29 22:27:05+00:00   \n",
      "\n",
      "                                       cleaned_tweet Stock Name  \n",
      "0  mainstream media has done an amazing job at br...       TSLA  \n",
      "1  tesla delivery estimates are at around 364k fr...       TSLA  \n",
      "2  3 even if i include 63.0m unvested rsus as of ...       TSLA  \n",
      "3  hahaha why are you still trying to stop tesla ...       TSLA  \n",
      "4  stop trying to kill kids you sad deranged old man       TSLA  \n"
     ]
    }
   ],
   "source": [
    "# Process stock_tweets\n",
    "stock_tweets_cleaned = stock_tweets.copy()\n",
    "\n",
    "# Convert date column to datetime\n",
    "stock_tweets_cleaned['Date'] = pd.to_datetime(stock_tweets_cleaned['Date'])\n",
    "\n",
    "# Clean the tweet text\n",
    "stock_tweets_cleaned['cleaned_tweet'] = stock_tweets_cleaned['Tweet'].apply(clean_text)\n",
    "\n",
    "# Remove rows with empty cleaned text\n",
    "stock_tweets_cleaned = stock_tweets_cleaned[stock_tweets_cleaned['cleaned_tweet'].str.len() > 0]\n",
    "\n",
    "# Extract date only (without time) for easier merging with stock prices\n",
    "stock_tweets_cleaned['date_only'] = stock_tweets_cleaned['Date'].dt.date\n",
    "\n",
    "\n",
    "print(f\"Original rows: {len(stock_tweets)}\")\n",
    "print(f\"Rows after cleaning: {len(stock_tweets_cleaned)}\")\n",
    "print(f\"\\nSample cleaned tweets:\")\n",
    "print(stock_tweets_cleaned[['Date', 'cleaned_tweet', 'Stock Name']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4f196c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "valid_tickers = set(stock_prices['Stock Name'].unique())\n",
    "print(len(valid_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "23666b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tickers in price data: 25\n",
      "Tickers: ['AAPL', 'AMD', 'AMZN', 'BA', 'BX', 'COST', 'CRM', 'DIS', 'ENPH', 'F', 'GOOG', 'INTC', 'KO', 'META', 'MSFT', 'NFLX', 'NIO', 'NOC', 'PG', 'PYPL', 'TSLA', 'TSM', 'VZ', 'XPEV', 'ZS']\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "unbalanced parenthesis at position 7",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTickers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msorted\u001b[39m(valid_tickers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 2. Apply the function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m stock_tweets_filtered = \u001b[43mselect_valid_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_tweets_cleaned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_tickers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m stock_tweets_filtered.to_csv(\u001b[33m'\u001b[39m\u001b[33mdata/processed/stock_tweets_filtered.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 3. View results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mselect_valid_tweets\u001b[39m\u001b[34m(stock_tweets_cleaned, valid_tickers)\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(valid_mentioned))  \u001b[38;5;66;03m# Return unique\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Add mentioned_tickers column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mmentioned_tickers\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcleaned_tweet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_valid_tickers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Add number of tickers mentioned\u001b[39;00m\n\u001b[32m     22\u001b[39m df_copy[\u001b[33m'\u001b[39m\u001b[33mnum_tickers\u001b[39m\u001b[33m'\u001b[39m] = df_copy[\u001b[33m'\u001b[39m\u001b[33mmentioned_tickers\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mselect_valid_tweets.<locals>.get_valid_tickers\u001b[39m\u001b[34m(cleaned_text)\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Extract ticker_xxx patterns and check if xxx is in valid_tickers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m mentioned = \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m$[a-z]+)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcleaned_text\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[32m     14\u001b[39m valid_mentioned = [t.upper() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m mentioned \u001b[38;5;28;01mif\u001b[39;00m t.upper() \u001b[38;5;129;01min\u001b[39;00m valid_tickers]\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(valid_mentioned))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/re/__init__.py:216\u001b[39m, in \u001b[36mfindall\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfindall\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    209\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m \n\u001b[32m    215\u001b[39m \u001b[33;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m.findall(string)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/re/__init__.py:294\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(pattern, flags)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    289\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mThe re.TEMPLATE/re.T flag is deprecated \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mas it is an undocumented flag \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mwithout an obvious purpose. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m               \u001b[33m\"\u001b[39m\u001b[33mDon\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use it.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    293\u001b[39m               \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m p = \u001b[43m_compiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags & DEBUG):\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) >= _MAXCACHE:\n\u001b[32m    297\u001b[39m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/re/_compiler.py:745\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(p, flags)\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[32m    744\u001b[39m     pattern = p\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     p = \u001b[43m_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    747\u001b[39m     pattern = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/dml/lib/python3.11/re/_parser.py:994\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(str, flags, state)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source.next \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m source.next == \u001b[33m\"\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m source.error(\u001b[33m\"\u001b[39m\u001b[33munbalanced parenthesis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m p.state.grouprefpos:\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m g >= p.state.groups:\n",
      "\u001b[31merror\u001b[39m: unbalanced parenthesis at position 7"
     ]
    }
   ],
   "source": [
    "def select_valid_tweets(stock_tweets_cleaned, valid_tickers):\n",
    "    \"\"\"\n",
    "    Expects a dataset of tweets with potentialstock tickers turned into ticker_XXX. \n",
    "    Goes trough the dataset, returns only the tweets containing valid tickers. And ads a \"single stock\" or \"multiple stocks\" column to the datset.\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = stock_tweets_cleaned.copy()\n",
    "\n",
    "    def get_valid_tickers(cleaned_text):\n",
    "        if pd.isna(cleaned_text):\n",
    "            return []\n",
    "        # Extract ticker_xxx patterns and check if xxx is in valid_tickers\n",
    "        mentioned = re.findall(r'$[a-z]+)', cleaned_text)   \n",
    "        valid_mentioned = [t.upper() for t in mentioned if t.upper() in valid_tickers]\n",
    "        return list(set(valid_mentioned))  # Return unique\n",
    "\n",
    "\n",
    "    # Add mentioned_tickers column\n",
    "    df_copy['mentioned_tickers'] = df_copy['cleaned_tweet'].apply(get_valid_tickers)\n",
    "    \n",
    "    # Add number of tickers mentioned\n",
    "    df_copy['num_tickers'] = df_copy['mentioned_tickers'].apply(len)\n",
    "    \n",
    "    # Add single/multiple stock flag\n",
    "    df_copy['ticker_flag'] = df_copy['num_tickers'].apply(\n",
    "        lambda x: 'single stock' if x == 1 else ('multiple stocks' if x > 1 else 'no valid ticker')\n",
    "    )\n",
    "    \n",
    "    # Filter: keep only tweets with at least one valid ticker\n",
    "    df_filtered = df_copy[df_copy['num_tickers'] > 0].copy()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Original tweets: {len(stock_tweets_cleaned)}\")\n",
    "    print(f\"Tweets with valid tickers: {len(df_filtered)}\")\n",
    "    print(f\"  - Single stock: {(df_filtered['ticker_flag'] == 'single stock').sum()}\")\n",
    "    print(f\"  - Multiple stocks: {(df_filtered['ticker_flag'] == 'multiple stocks').sum()}\")\n",
    "    print(f\"Removed (no valid tickers): {len(stock_tweets_cleaned) - len(df_filtered)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# 1. Get valid tickers from yfinance data\n",
    "valid_tickers = set(stock_prices['Stock Name'].unique())\n",
    "print(f\"Valid tickers in price data: {len(valid_tickers)}\")\n",
    "print(f\"Tickers: {sorted(valid_tickers)}\\n\")\n",
    "\n",
    "# 2. Apply the function\n",
    "stock_tweets_filtered = select_valid_tweets(stock_tweets_cleaned, valid_tickers)\n",
    "stock_tweets_filtered.to_csv('data/processed/stock_tweets_filtered.csv', index=False)\n",
    "# 3. View results\n",
    "print(\"\\nSample results:\")\n",
    "print(stock_tweets_filtered[['cleaned_tweet', 'mentioned_tickers', 'ticker_flag']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "79a1d29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 6300\n",
      "Rows after processing: 6275\n",
      "\n",
      "Sample processed data:\n",
      "        Date        Open        High         Low       Close   Adj Close  \\\n",
      "1 2021-10-01  259.466675  260.260010  254.529999  258.406677  258.406677   \n",
      "2 2021-10-04  265.500000  268.989990  258.706665  260.510010  260.510010   \n",
      "3 2021-10-05  261.600006  265.769989  258.066681  260.196655  260.196655   \n",
      "4 2021-10-06  258.733337  262.220001  257.739990  260.916656  260.916656   \n",
      "5 2021-10-07  261.820007  268.333344  261.126678  264.536682  264.536682   \n",
      "\n",
      "     Volume Stock Name  daily_return  price_range   date_only  \n",
      "1  51094200       TSLA     -0.000335     5.730011  2021-10-01  \n",
      "2  91449900       TSLA      0.008140    10.283325  2021-10-04  \n",
      "3  55297800       TSLA     -0.001203     7.703308  2021-10-05  \n",
      "4  43898400       TSLA      0.002767     4.480011  2021-10-06  \n",
      "5  57587400       TSLA      0.013874     7.206665  2021-10-07  \n"
     ]
    }
   ],
   "source": [
    "# Process stock_prices\n",
    "stock_prices_cleaned = stock_prices.copy()\n",
    "\n",
    "# Date is already converted to datetime in previous cell\n",
    "# Calculate additional features\n",
    "stock_prices_cleaned['daily_return'] = stock_prices_cleaned.groupby('Stock Name')['Close'].pct_change()\n",
    "stock_prices_cleaned['price_range'] = stock_prices_cleaned['High'] - stock_prices_cleaned['Low']\n",
    "stock_prices_cleaned['date_only'] = stock_prices_cleaned['Date'].dt.date\n",
    "\n",
    "# Handle any missing values\n",
    "stock_prices_cleaned = stock_prices_cleaned.dropna()\n",
    "\n",
    "print(f\"Original rows: {len(stock_prices)}\")\n",
    "print(f\"Rows after processing: {len(stock_prices_cleaned)}\")\n",
    "print(f\"\\nSample processed data:\")\n",
    "print(stock_prices_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e1ac8",
   "metadata": {},
   "source": [
    "## 4. Merge Datasets (Tweets + Stock Prices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cd3df13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (63497, 11)\n",
      "\n",
      "Sample merged data:\n",
      "                       Date  \\\n",
      "0 2022-09-29 23:41:16+00:00   \n",
      "1 2022-09-29 23:24:43+00:00   \n",
      "2 2022-09-29 23:18:08+00:00   \n",
      "3 2022-09-29 22:40:07+00:00   \n",
      "4 2022-09-29 22:27:05+00:00   \n",
      "\n",
      "                                       cleaned_tweet Stock Name Company Name  \\\n",
      "0  mainstream media has done an amazing job at br...       TSLA  Tesla, Inc.   \n",
      "1  tesla delivery estimates are at around 364k fr...       TSLA  Tesla, Inc.   \n",
      "2  3 even if i include 63.0m unvested rsus as of ...       TSLA  Tesla, Inc.   \n",
      "3  hahaha why are you still trying to stop tesla ...       TSLA  Tesla, Inc.   \n",
      "4  stop trying to kill kids you sad deranged old man       TSLA  Tesla, Inc.   \n",
      "\n",
      "        Open        High         Low       Close    Volume  daily_return  \\\n",
      "0  282.76001  283.649994  265.779999  268.209991  77620600     -0.068101   \n",
      "1  282.76001  283.649994  265.779999  268.209991  77620600     -0.068101   \n",
      "2  282.76001  283.649994  265.779999  268.209991  77620600     -0.068101   \n",
      "3  282.76001  283.649994  265.779999  268.209991  77620600     -0.068101   \n",
      "4  282.76001  283.649994  265.779999  268.209991  77620600     -0.068101   \n",
      "\n",
      "   price_range  \n",
      "0    17.869995  \n",
      "1    17.869995  \n",
      "2    17.869995  \n",
      "3    17.869995  \n",
      "4    17.869995  \n",
      "\n",
      "Date range: 2021-10-01 00:01:52+00:00 to 2022-09-29 23:41:16+00:00\n"
     ]
    }
   ],
   "source": [
    "# Merge stock tweets with stock prices based on date and stock name\n",
    "merged_data = pd.merge(\n",
    "    stock_tweets_cleaned,\n",
    "    stock_prices_cleaned,\n",
    "    left_on=['date_only', 'Stock Name'],\n",
    "    right_on=['date_only', 'Stock Name'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "merged_data = merged_data[[\n",
    "    'Date_x', 'cleaned_tweet', 'Stock Name', 'Company Name',\n",
    "    'Open', 'High', 'Low', 'Close', 'Volume', 'daily_return', 'price_range'\n",
    "]]\n",
    "\n",
    "# Rename Date_x to Date\n",
    "merged_data = merged_data.rename(columns={'Date_x': 'Date'})\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_data.shape}\")\n",
    "print(f\"\\nSample merged data:\")\n",
    "print(merged_data.head())\n",
    "print(f\"\\nDate range: {merged_data['Date'].min()} to {merged_data['Date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fa133a",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a91ff2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets to the processed folder\n",
    "sentiment_labeled_data_cleaned.to_csv('data/processed/labeled_sentiment_tweets.csv', index=False)\n",
    "stock_tweets_cleaned.to_csv('data/processed/cleaned_stock_tweets.csv', index=False)\n",
    "stock_prices_cleaned.to_csv('data/processed/stock_prices_features.csv', index=False)\n",
    "merged_data.to_csv('data/processed/tweets_with_stock_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34391ef",
   "metadata": {},
   "source": [
    "## 6. Data Summary & Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6573a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATASET SUMMARY:\n",
      "\n",
      "1. Labeled Sentiment Tweets: 5,791 rows\n",
      "   - Purpose: Training sentiment model\n",
      "   - Sentiment distribution: {1: 3685, -1: 2106}\n",
      "\n",
      "2. Cleaned Stock Tweets: 80,792 rows\n",
      "   - Purpose: Applying sentiment model to real stock data\n",
      "   - Stocks: TSLA, MSFT, PG, META, AMZN, GOOG, AMD, AAPL, NFLX, TSM, KO, F, COST, DIS, VZ, CRM, INTC, BA, BX, NOC, PYPL, ENPH, NIO, ZS, XPEV\n",
      "\n",
      "3. Stock Prices with Features: 6,275 rows\n",
      "   - Purpose: Financial features for prediction\n",
      "   - Features: Open, High, Low, Close, Volume, daily_return, price_range\n",
      "\n",
      "4. Merged Tweets + Stock Data: 63,497 rows\n",
      "   - Purpose: Combined dataset for sentiment-based stock prediction\n",
      "   - Ready for model training!\n"
     ]
    }
   ],
   "source": [
    "# Summary of processed data\n",
    "print(\"\\n DATASET SUMMARY:\")\n",
    "print(f\"\\n1. Labeled Sentiment Tweets: {len(sentiment_labeled_data_cleaned):,} rows\")\n",
    "print(f\"   - Purpose: Training sentiment model\")\n",
    "print(f\"   - Sentiment distribution: {dict(sentiment_labeled_data_cleaned['Sentiment'].value_counts())}\")\n",
    "\n",
    "print(f\"\\n2. Cleaned Stock Tweets: {len(stock_tweets_cleaned):,} rows\")\n",
    "print(f\"   - Purpose: Applying sentiment model to real stock data\")\n",
    "print(f\"   - Stocks: {', '.join(stock_tweets_cleaned['Stock Name'].unique())}\")\n",
    "\n",
    "print(f\"\\n3. Stock Prices with Features: {len(stock_prices_cleaned):,} rows\")\n",
    "print(f\"   - Purpose: Financial features for prediction\")\n",
    "print(f\"   - Features: Open, High, Low, Close, Volume, daily_return, price_range\")\n",
    "\n",
    "print(f\"\\n4. Merged Tweets + Stock Data: {len(merged_data):,} rows\")\n",
    "print(f\"   - Purpose: Combined dataset for sentiment-based stock prediction\")\n",
    "print(f\"   - Ready for model training!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c546a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ac64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
