{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Sentiment Analysis with FinBERT\n",
        "\n",
        "This notebook trains a custom binary sentiment classifier using the ProsusAI/finbert base model.\n",
        "- **Model**: ProsusAI/finbert (base model, pre-trained on financial text)\n",
        "- **Task**: Binary classification (positive vs negative sentiment)\n",
        "- **Dataset**: Stock market tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. **Imports** - All required libraries\n",
        "2. **Data Preprocessing** - Text cleaning and label conversion\n",
        "3. **Load and Explore Data** - Dataset loading and exploration\n",
        "4. **Custom Dataset Class** - PyTorch dataset implementation\n",
        "5. **Model Architecture** - FinBERT backbone + custom binary head\n",
        "6. **Training Utilities** - Training loop, metrics, plotting functions\n",
        "7. **Initialize Model and Data** - Setup for manual training (optional)\n",
        "8. **Training Strategy** - Two-phase training approach (optional manual run)\n",
        "9. **Complete Training & Testing Pipeline** - `train_and_evaluate()` function\n",
        "10. **Test Function** - Comprehensive testing with confusion matrix\n",
        "11. **Inference Function** - Predict sentiment on custom text\n",
        "12. **Example Predictions** - Test the trained model\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text for sentiment analysis.\"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)                      # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)                          # Remove mentions\n",
        "    text = re.sub(r\"^user:\\s*\", \"\", text, flags=re.IGNORECASE)  # Remove 'user:' prefix\n",
        "    text = re.sub(r\"^user\\s*\", \"\", text, flags=re.IGNORECASE)   # Remove 'user' prefix\n",
        "    text = re.sub(r\"[\\\"]+\", \"\", text)                        # Remove quotes\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "def labels_zero_one(y: int) -> int:\n",
        "    \"\"\"Convert sentiment labels from {-1, 1} to {0, 1}.\"\"\"\n",
        "    # input: -1 (negative) or 1 (positive)\n",
        "    # output: 0 (negative) or 1 (positive)\n",
        "    return 1 if int(y) == 1 else 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset size: 5791 samples\n",
            "\n",
            "Sentiment distribution:\n",
            "Sentiment\n",
            " 1    3685\n",
            "-1    2106\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First few samples:\n",
            "                                                Text  Sentiment\n",
            "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
            "1  AAP MOVIE. 55% return for the FEA/GEED indicat...          1\n",
            "2  I'd be afraid to short AMZN - they are looking...          1\n",
            "3                                    MNTA Over 12.00          1\n",
            "4                                      OI Over 21.37          1\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"data/stock_data.csv\")\n",
        "df[\"Text\"] = df[\"Text\"].astype(str).apply(clean_text)\n",
        "\n",
        "print(f\"Dataset size: {len(df)} samples\")\n",
        "print(f\"\\nOriginal Sentiment distribution:\")\n",
        "print(df[\"Sentiment\"].value_counts())\n",
        "print(f\"\\nClass distribution:\")\n",
        "pos_count = (df[\"Sentiment\"] == 1).sum()\n",
        "neg_count = (df[\"Sentiment\"] == -1).sum()\n",
        "print(f\"  Positive (1):  {pos_count} ({pos_count/len(df)*100:.1f}%)\")\n",
        "print(f\"  Negative (-1): {neg_count} ({neg_count/len(df)*100:.1f}%)\")\n",
        "print(f\"  Imbalance ratio: {pos_count/neg_count:.2f}:1\")\n",
        "print(f\"\\nFirst few samples:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Handle Class Imbalance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def balance_dataset(df, method='undersample', random_state=42):\n",
        "    \"\"\"\n",
        "    Balance dataset by handling class imbalance.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with 'Text' and 'Sentiment' columns\n",
        "        method: 'undersample' (remove majority), 'oversample' (duplicate minority), or 'none'\n",
        "        random_state: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        Balanced DataFrame\n",
        "    \"\"\"\n",
        "    pos_df = df[df[\"Sentiment\"] == 1]\n",
        "    neg_df = df[df[\"Sentiment\"] == -1]\n",
        "    \n",
        "    if method == 'undersample':\n",
        "        # Undersample majority class to match minority\n",
        "        min_count = min(len(pos_df), len(neg_df))\n",
        "        pos_balanced = pos_df.sample(n=min_count, random_state=random_state)\n",
        "        neg_balanced = neg_df.sample(n=min_count, random_state=random_state)\n",
        "        \n",
        "    elif method == 'oversample':\n",
        "        # Oversample minority class to match majority\n",
        "        max_count = max(len(pos_df), len(neg_df))\n",
        "        pos_balanced = pos_df.sample(n=max_count, replace=True, random_state=random_state)\n",
        "        neg_balanced = neg_df.sample(n=max_count, replace=True, random_state=random_state)\n",
        "        \n",
        "    elif method == 'none':\n",
        "        return df\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}. Use 'undersample', 'oversample', or 'none'\")\n",
        "    \n",
        "    # Combine and shuffle\n",
        "    balanced_df = pd.concat([pos_balanced, neg_balanced], ignore_index=True)\n",
        "    balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "    \n",
        "    return balanced_df\n",
        "\n",
        "\n",
        "def compute_class_weights(df):\n",
        "    \"\"\"\n",
        "    Compute class weights for imbalanced dataset.\n",
        "    Useful for weighted loss during training.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with 'Sentiment' column\n",
        "    \n",
        "    Returns:\n",
        "        pos_weight: Weight for positive class (as torch tensor)\n",
        "    \"\"\"\n",
        "    pos_count = (df[\"Sentiment\"] == 1).sum()\n",
        "    neg_count = (df[\"Sentiment\"] == -1).sum()\n",
        "    \n",
        "    # Weight for positive class (how much to emphasize minority class)\n",
        "    pos_weight = neg_count / pos_count\n",
        "    \n",
        "    return torch.tensor([pos_weight], dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced Sentiment distribution:\n",
            "Sentiment\n",
            " 1    2106\n",
            "-1    2106\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Balance the dataset using undersampling (removing excess positive samples)\n",
        "df_balanced = balance_dataset(df, method='undersample', random_state=42)\n",
        "\n",
        "print(\"Balanced Sentiment distribution:\")\n",
        "print(df_balanced[\"Sentiment\"].value_counts())\n",
        "\n",
        "# Save balanced dataset\n",
        "df_balanced.to_csv(\"data/stock_data_balanced.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Options for Handling Imbalanced Data\n",
        "\n",
        "There are multiple strategies to handle class imbalance:\n",
        "\n",
        "1. **Undersampling (Used Above)** ✅\n",
        "   - Randomly removes samples from majority class to match minority\n",
        "   - Pros: Fast, prevents model from being biased towards majority\n",
        "   - Cons: Loses data (went from 5,791 → 4,212 samples)\n",
        "\n",
        "2. **Oversampling** \n",
        "   - Duplicates minority class samples to match majority\n",
        "   - Pros: Keeps all original data\n",
        "   - Cons: Risk of overfitting on duplicated samples\n",
        "   - To use: `balance_dataset(df, method='oversample')`\n",
        "\n",
        "3. **Class Weights** ⚖️\n",
        "   - Uses weighted loss to penalize errors on minority class more\n",
        "   - Pros: Keeps all data, no duplication\n",
        "   - Cons: Needs tuning\n",
        "   - To use: Set `use_class_weights=True` in `train_and_evaluate()`\n",
        "\n",
        "**Current strategy:** Undersampling (balanced dataset with 2,106 samples per class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Custom Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinancialSentimentDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for financial sentiment analysis.\"\"\"\n",
        "    \n",
        "    def __init__(self, csvPath: Path, tokenizer, max_len: int = 128):\n",
        "        csvPath = Path(csvPath)\n",
        "        if not csvPath.exists():\n",
        "            raise ValueError(f\"CSV not found: {csvPath}\")\n",
        "\n",
        "        df = pd.read_csv(csvPath)\n",
        "        # Expect columns: Text, Sentiment\n",
        "        self.texts = [clean_text(t) for t in df[\"Text\"].astype(str).tolist()]\n",
        "        self.labels = [labels_zero_one(y) for y in df[\"Sentiment\"].tolist()]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxLen = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,          # Cut off long texts\n",
        "            padding='max_length',     # Pad shorter texts\n",
        "            max_length=self.maxLen,\n",
        "            return_tensors='pt'       # PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Architecture\n",
        "\n",
        "We use a custom binary classification head on top of the FinBERT base model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinbertBackbone(nn.Module):\n",
        "    \"\"\"FinBERT encoder backbone.\"\"\"\n",
        "    \n",
        "    def __init__(self, modelName: str = \"ProsusAI/finbert\"):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(modelName)\n",
        "        self.hiddenSize = self.encoder.config.hidden_size  # 768 for BERT-base\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        cls = out.last_hidden_state[:, 0]  # Extract [CLS] token embedding\n",
        "        return cls  # [batch_size, hidden_size]\n",
        "\n",
        "\n",
        "class BinaryHead(nn.Module):\n",
        "    \"\"\"Binary classification head.\"\"\"\n",
        "    \n",
        "    def __init__(self, inFeatures: int, pDrop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(pDrop)\n",
        "        self.fc = nn.Linear(inFeatures, 1)  # Single logit for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x).squeeze(-1)  # [batch_size]\n",
        "        return logits\n",
        "\n",
        "\n",
        "class FinbertBinaryClf(nn.Module):\n",
        "    \"\"\"Complete model: FinBERT backbone + custom binary classification head.\"\"\"\n",
        "    \n",
        "    def __init__(self, modelName: str = \"ProsusAI/finbert\", pDrop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.backbone = FinbertBackbone(modelName)\n",
        "        self.head = BinaryHead(self.backbone.hiddenSize, pDrop)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        feats = self.backbone(input_ids, attention_mask)\n",
        "        logits = self.head(feats)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getLoaders(csvPath, tokenizer, maxLen=128, batchSize=8, valFrac=0.2, seed=42):\n",
        "    \"\"\"Create train and validation data loaders.\"\"\"\n",
        "    ds = FinancialSentimentDataset(csvPath, tokenizer, maxLen)\n",
        "    valLen = int(len(ds) * valFrac)\n",
        "    trainLen = len(ds) - valLen\n",
        "    gen = torch.Generator().manual_seed(seed)\n",
        "    trainDs, valDs = random_split(ds, [trainLen, valLen], generator=gen)\n",
        "    return (\n",
        "        DataLoader(trainDs, batch_size=batchSize, shuffle=True),\n",
        "        DataLoader(valDs, batch_size=batchSize, shuffle=False),\n",
        "    )\n",
        "\n",
        "\n",
        "def step(model, batch, device, posWeight=None, train=True, optimizer=None, clip=1.0):\n",
        "    \"\"\"Perform one training/validation step.\"\"\"\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"label\"].float().to(device)  # 0/1 as float for BCE\n",
        "\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    \n",
        "    # Compute loss with optional class weighting\n",
        "    if posWeight is None:\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "    else:\n",
        "        bce = nn.BCEWithLogitsLoss(pos_weight=posWeight)\n",
        "        loss = bce(logits, labels)\n",
        "\n",
        "    # Backward pass and optimization (only in training mode)\n",
        "    if train:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    # Get predictions\n",
        "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    y_true = labels.detach().cpu().numpy().astype(int)\n",
        "    \n",
        "    return loss.item(), preds, y_true\n",
        "\n",
        "\n",
        "def runEpoch(model, loader, device, train, optimizer=None, posWeight=None):\n",
        "    \"\"\"Run one epoch of training or validation.\"\"\"\n",
        "    model.train(train)\n",
        "    losses, allPreds, allTrue = [], [], []\n",
        "    \n",
        "    for batch in loader:\n",
        "        loss, preds, y_true = step(model, batch, device, posWeight, train, optimizer)\n",
        "        losses.append(loss)\n",
        "        allPreds.extend(preds.tolist())\n",
        "        allTrue.extend(y_true.tolist())\n",
        "    \n",
        "    acc = accuracy_score(allTrue, allPreds)\n",
        "    f1 = f1_score(allTrue, allPreds)\n",
        "    avg_loss = float(sum(losses) / max(1, len(losses)))\n",
        "    \n",
        "    return avg_loss, acc, f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics over epochs.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    # Plot Loss\n",
        "    axes[0].plot(history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot Accuracy\n",
        "    axes[1].plot(history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
        "    axes[1].plot(history['val_acc'], 'r-', label='Val Accuracy', linewidth=2)\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot F1 Score\n",
        "    axes[2].plot(history['train_f1'], 'b-', label='Train F1', linewidth=2)\n",
        "    axes[2].plot(history['val_f1'], 'r-', label='Val F1', linewidth=2)\n",
        "    axes[2].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[2].set_ylabel('F1 Score', fontsize=12)\n",
        "    axes[2].set_title('Training and Validation F1 Score', fontsize=14, fontweight='bold')\n",
        "    axes[2].legend(fontsize=10)\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    \"\"\"Evaluate model and return detailed metrics.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_true, all_probs = [], [], []\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"label\"].float().to(device)\n",
        "            \n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            probs = torch.sigmoid(logits).cpu().numpy()\n",
        "            preds = (probs >= 0.5).astype(int)\n",
        "            \n",
        "            all_probs.extend(probs.tolist())\n",
        "            all_preds.extend(preds.tolist())\n",
        "            all_true.extend(labels.cpu().numpy().astype(int).tolist())\n",
        "    \n",
        "    avg_loss = total_loss / len(loader)\n",
        "    acc = accuracy_score(all_true, all_preds)\n",
        "    f1 = f1_score(all_true, all_preds)\n",
        "    \n",
        "    return avg_loss, acc, f1, all_preds, all_true, all_probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer from ProsusAI/finbert...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7399dfda6406455fab07b48d37e8f338",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\carlk\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d37c216851b4b8cab9383ab5a2017c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8840245c8fe344669169553e6da564fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b6456cae49a4c399adb752122eff1f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating data loaders...\n",
            "Train batches: 580\n",
            "Validation batches: 145\n",
            "\n",
            "Using device: cpu\n",
            "\n",
            "Loading model from ProsusAI/finbert...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f912e610449d4bf29db40ec4db1b23e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c7ee601b87d40d1a0f52699b5152aa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Configuration\n",
        "CSV_PATH = \"data/stock_data.csv\"\n",
        "MODEL_NAME = \"ProsusAI/finbert\"  # Base FinBERT model\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 8\n",
        "VAL_FRACTION = 0.2\n",
        "DROPOUT = 0.1\n",
        "SEED = 42\n",
        "\n",
        "# Initialize tokenizer\n",
        "print(f\"Loading tokenizer from {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Create data loaders\n",
        "print(f\"Creating data loaders...\")\n",
        "trainLoader, valLoader = getLoaders(\n",
        "    CSV_PATH, \n",
        "    tokenizer, \n",
        "    maxLen=MAX_LEN, \n",
        "    batchSize=BATCH_SIZE,\n",
        "    valFrac=VAL_FRACTION,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(trainLoader)}\")\n",
        "print(f\"Validation batches: {len(valLoader)}\")\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "print(f\"\\nLoading model from {MODEL_NAME}...\")\n",
        "model = FinbertBinaryClf(MODEL_NAME, pDrop=DROPOUT).to(device)\n",
        "print(f\"Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Strategy: Two-Phase Approach\n",
        "\n",
        "### Phase 1: Warmup (Freeze Encoder)\n",
        "Train only the custom classification head while keeping the encoder frozen.\n",
        "\n",
        "### Phase 2: Fine-tuning (Unfreeze Encoder)\n",
        "Fine-tune the entire model with a lower learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "PHASE 1: Warmup - Training classification head only\n",
            "============================================================\n",
            "[Warmup Epoch 0]\n",
            "  Train: loss=0.5761, acc=0.698, f1=0.790\n",
            "  Val:   loss=0.5980, acc=0.708, f1=0.786\n"
          ]
        }
      ],
      "source": [
        "# PHASE 1: WARMUP - Train only the head\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: Warmup - Training classification head only\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize history tracking\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'train_f1': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': [],\n",
        "    'val_f1': []\n",
        "}\n",
        "\n",
        "# Freeze encoder parameters\n",
        "for p in model.backbone.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Optimizer for head only\n",
        "optimizer = AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()), \n",
        "    lr=2e-4, \n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Warmup training\n",
        "WARMUP_EPOCHS = 1\n",
        "for epoch in range(WARMUP_EPOCHS):\n",
        "    trLoss, trAcc, trF1 = runEpoch(model, trainLoader, device, train=True, optimizer=optimizer)\n",
        "    vaLoss, vaAcc, vaF1 = runEpoch(model, valLoader, device, train=False)\n",
        "    \n",
        "    # Track metrics\n",
        "    history['train_loss'].append(trLoss)\n",
        "    history['train_acc'].append(trAcc)\n",
        "    history['train_f1'].append(trF1)\n",
        "    history['val_loss'].append(vaLoss)\n",
        "    history['val_acc'].append(vaAcc)\n",
        "    history['val_f1'].append(vaF1)\n",
        "    \n",
        "    print(f\"[Warmup Epoch {epoch}]\")\n",
        "    print(f\"  Train: loss={trLoss:.4f}, acc={trAcc:.3f}, f1={trF1:.3f}\")\n",
        "    print(f\"  Val:   loss={vaLoss:.4f}, acc={vaAcc:.3f}, f1={vaF1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PHASE 2: FINE-TUNING - Train entire model\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: Fine-tuning - Training entire model\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Unfreeze all encoder parameters\n",
        "for p in model.backbone.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# New optimizer with lower learning rate for entire model\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "# Main training\n",
        "TRAIN_EPOCHS = 3\n",
        "best_f1 = 0.0\n",
        "\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    trLoss, trAcc, trF1 = runEpoch(model, trainLoader, device, train=True, optimizer=optimizer)\n",
        "    vaLoss, vaAcc, vaF1 = runEpoch(model, valLoader, device, train=False)\n",
        "    \n",
        "    # Track metrics\n",
        "    history['train_loss'].append(trLoss)\n",
        "    history['train_acc'].append(trAcc)\n",
        "    history['train_f1'].append(trF1)\n",
        "    history['val_loss'].append(vaLoss)\n",
        "    history['val_acc'].append(vaAcc)\n",
        "    history['val_f1'].append(vaF1)\n",
        "    \n",
        "    print(f\"[Epoch {epoch}]\")\n",
        "    print(f\"  Train: loss={trLoss:.4f}, acc={trAcc:.3f}, f1={trF1:.3f}\")\n",
        "    print(f\"  Val:   loss={vaLoss:.4f}, acc={vaAcc:.3f}, f1={vaF1:.3f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if vaF1 > best_f1:\n",
        "        best_f1 = vaF1\n",
        "        torch.save(model.state_dict(), \"finbert_custom_head_best.pt\")\n",
        "        print(f\"  → New best model saved! (F1={best_f1:.3f})\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), \"finbert_custom_head_final.pt\")\n",
        "print(f\"\\n✓ Training complete! Best validation F1: {best_f1:.3f}\")\n",
        "\n",
        "# Plot training history\n",
        "print(\"\\nGenerating training plots...\")\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Complete Training & Testing Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_and_evaluate(\n",
        "    csv_path, \n",
        "    model_name=\"ProsusAI/finbert\",\n",
        "    max_len=128,\n",
        "    batch_size=8,\n",
        "    val_frac=0.2,\n",
        "    warmup_epochs=1,\n",
        "    train_epochs=3,\n",
        "    warmup_lr=2e-4,\n",
        "    train_lr=2e-5,\n",
        "    dropout=0.1,\n",
        "    seed=42,\n",
        "    save_best=True,\n",
        "    use_class_weights=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete training and evaluation pipeline.\n",
        "    \n",
        "    Args:\n",
        "        csv_path: Path to CSV data file\n",
        "        model_name: HuggingFace model name\n",
        "        max_len: Maximum sequence length\n",
        "        batch_size: Training batch size\n",
        "        val_frac: Validation fraction\n",
        "        warmup_epochs: Number of warmup epochs (head only)\n",
        "        train_epochs: Number of fine-tuning epochs\n",
        "        warmup_lr: Learning rate for warmup phase\n",
        "        train_lr: Learning rate for fine-tuning phase\n",
        "        dropout: Dropout probability\n",
        "        seed: Random seed\n",
        "        save_best: Whether to save best model\n",
        "        use_class_weights: Use weighted loss for imbalanced classes (set to False if using balanced data)\n",
        "    \n",
        "    Returns:\n",
        "        model: Trained model\n",
        "        history: Training history dictionary\n",
        "        test_results: Test evaluation results\n",
        "    \"\"\"\n",
        "    \n",
        "    # 1. Setup\n",
        "    print(\"🚀 STARTING TRAINING PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "    \n",
        "    # 2. Load tokenizer and data\n",
        "    print(f\"\\n📚 Loading data and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    trainLoader, valLoader = getLoaders(\n",
        "        csv_path, tokenizer, max_len, batch_size, val_frac, seed\n",
        "    )\n",
        "    print(f\"  Train batches: {len(trainLoader)}\")\n",
        "    print(f\"  Val batches: {len(valLoader)}\")\n",
        "    \n",
        "    # 3. Check for class weights\n",
        "    pos_weight = None\n",
        "    if use_class_weights:\n",
        "        df_temp = pd.read_csv(csv_path)\n",
        "        pos_weight = compute_class_weights(df_temp).to(device)\n",
        "        print(f\"  ⚖️  Using class weights: pos_weight={pos_weight.item():.3f}\")\n",
        "    else:\n",
        "        print(f\"  ℹ️  Not using class weights (assuming balanced data)\")\n",
        "    \n",
        "    # 3. Initialize model\n",
        "    print(f\"\\n🤖 Initializing model: {model_name}\")\n",
        "    model = FinbertBinaryClf(model_name, pDrop=dropout).to(device)\n",
        "    \n",
        "    # 4. Track metrics\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_f1': []\n",
        "    }\n",
        "    \n",
        "    # 5. PHASE 1: Warmup\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PHASE 1: Warmup Training (Head Only) - {warmup_epochs} epoch(s)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    for p in model.backbone.parameters():\n",
        "        p.requires_grad = False\n",
        "    \n",
        "    optimizer = AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()), \n",
        "        lr=warmup_lr, weight_decay=0.01\n",
        "    )\n",
        "    \n",
        "    for epoch in range(warmup_epochs):\n",
        "        trLoss, trAcc, trF1 = runEpoch(model, trainLoader, device, train=True, optimizer=optimizer, posWeight=pos_weight)\n",
        "        vaLoss, vaAcc, vaF1 = runEpoch(model, valLoader, device, train=False, posWeight=pos_weight)\n",
        "        \n",
        "        history['train_loss'].append(trLoss)\n",
        "        history['train_acc'].append(trAcc)\n",
        "        history['train_f1'].append(trF1)\n",
        "        history['val_loss'].append(vaLoss)\n",
        "        history['val_acc'].append(vaAcc)\n",
        "        history['val_f1'].append(vaF1)\n",
        "        \n",
        "        print(f\"[Warmup {epoch}] Train: loss={trLoss:.4f} acc={trAcc:.3f} f1={trF1:.3f} | \"\n",
        "              f\"Val: loss={vaLoss:.4f} acc={vaAcc:.3f} f1={vaF1:.3f}\")\n",
        "    \n",
        "    # 6. PHASE 2: Fine-tuning\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PHASE 2: Fine-tuning (Full Model) - {train_epochs} epoch(s)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    for p in model.backbone.parameters():\n",
        "        p.requires_grad = True\n",
        "    \n",
        "    optimizer = AdamW(model.parameters(), lr=train_lr, weight_decay=0.01)\n",
        "    best_f1 = 0.0\n",
        "    \n",
        "    for epoch in range(train_epochs):\n",
        "        trLoss, trAcc, trF1 = runEpoch(model, trainLoader, device, train=True, optimizer=optimizer, posWeight=pos_weight)\n",
        "        vaLoss, vaAcc, vaF1 = runEpoch(model, valLoader, device, train=False, posWeight=pos_weight)\n",
        "        \n",
        "        history['train_loss'].append(trLoss)\n",
        "        history['train_acc'].append(trAcc)\n",
        "        history['train_f1'].append(trF1)\n",
        "        history['val_loss'].append(vaLoss)\n",
        "        history['val_acc'].append(vaAcc)\n",
        "        history['val_f1'].append(vaF1)\n",
        "        \n",
        "        print(f\"[Epoch {epoch}] Train: loss={trLoss:.4f} acc={trAcc:.3f} f1={trF1:.3f} | \"\n",
        "              f\"Val: loss={vaLoss:.4f} acc={vaAcc:.3f} f1={vaF1:.3f}\")\n",
        "        \n",
        "        if save_best and vaF1 > best_f1:\n",
        "            best_f1 = vaF1\n",
        "            torch.save(model.state_dict(), \"finbert_custom_head_best.pt\")\n",
        "            print(f\"  ✓ Best model saved (F1={best_f1:.3f})\")\n",
        "    \n",
        "    # 7. Save final model\n",
        "    torch.save(model.state_dict(), \"finbert_custom_head_final.pt\")\n",
        "    print(f\"\\n✓ Training complete! Best F1: {best_f1:.3f}\")\n",
        "    \n",
        "    # 8. Plot training history\n",
        "    print(f\"\\n📊 Generating plots...\")\n",
        "    plot_training_history(history)\n",
        "    \n",
        "    # 9. Evaluate on validation set\n",
        "    print(f\"\\n🧪 Final Evaluation:\")\n",
        "    test_results = test_model(model, valLoader, device, show_confusion_matrix=True)\n",
        "    \n",
        "    return model, history, test_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the complete training and evaluation pipeline\n",
        "# Using the balanced dataset for better model performance\n",
        "model, history, test_results = train_and_evaluate(\n",
        "    csv_path=\"data/stock_data_balanced.csv\",  # Using balanced dataset\n",
        "    model_name=\"ProsusAI/finbert\",\n",
        "    max_len=128,\n",
        "    batch_size=8,\n",
        "    val_frac=0.2,\n",
        "    warmup_epochs=1,\n",
        "    train_epochs=3,\n",
        "    warmup_lr=2e-4,\n",
        "    train_lr=2e-5,\n",
        "    dropout=0.1,\n",
        "    seed=42,\n",
        "    save_best=True,\n",
        "    use_class_weights=False  # False because we're using balanced data\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test Function with Detailed Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, device, show_confusion_matrix=True):\n",
        "    \"\"\"\n",
        "    Comprehensive test function with detailed metrics and visualizations.\n",
        "    \n",
        "    Args:\n",
        "        model: Trained model\n",
        "        test_loader: DataLoader for test data\n",
        "        device: Device to run on\n",
        "        show_confusion_matrix: Whether to plot confusion matrix\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with test metrics\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"TESTING MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Evaluate model\n",
        "    test_loss, test_acc, test_f1, preds, labels, probs = evaluate_model(model, test_loader, device)\n",
        "    \n",
        "    # Print metrics\n",
        "    print(f\"\\n📊 Test Results:\")\n",
        "    print(f\"  Loss:     {test_loss:.4f}\")\n",
        "    print(f\"  Accuracy: {test_acc:.3f} ({test_acc*100:.1f}%)\")\n",
        "    print(f\"  F1 Score: {test_f1:.3f}\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(f\"\\n📋 Detailed Classification Report:\")\n",
        "    print(classification_report(labels, preds, target_names=['Negative', 'Positive'], digits=3))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    if show_confusion_matrix:\n",
        "        cm = confusion_matrix(labels, preds)\n",
        "        \n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "        plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
        "        plt.colorbar()\n",
        "        \n",
        "        classes = ['Negative', 'Positive']\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, fontsize=12)\n",
        "        plt.yticks(tick_marks, classes, fontsize=12)\n",
        "        \n",
        "        # Add text annotations\n",
        "        thresh = cm.max() / 2.\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                        ha=\"center\", va=\"center\",\n",
        "                        color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                        fontsize=14)\n",
        "        \n",
        "        plt.ylabel('True Label', fontsize=12)\n",
        "        plt.xlabel('Predicted Label', fontsize=12)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "    \n",
        "    return {\n",
        "        'loss': test_loss,\n",
        "        'accuracy': test_acc,\n",
        "        'f1': test_f1,\n",
        "        'predictions': preds,\n",
        "        'labels': labels,\n",
        "        'probabilities': probs\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model on validation set\n",
        "test_results = test_model(model, valLoader, device, show_confusion_matrix=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predictTexts(texts, tokenizer, model, maxLen=128, threshold=0.5, device=None):\n",
        "    \"\"\"Predict sentiment for one or more texts.\n",
        "    \n",
        "    Args:\n",
        "        texts: Single text string or list of texts\n",
        "        tokenizer: HuggingFace tokenizer\n",
        "        model: Trained model\n",
        "        maxLen: Maximum sequence length\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "        device: Device to run inference on\n",
        "    \n",
        "    Returns:\n",
        "        labels: Binary labels (0=negative, 1=positive)\n",
        "        probs: Probability scores\n",
        "    \"\"\"\n",
        "    device = device or next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    \n",
        "    enc = tokenizer(\n",
        "        texts, \n",
        "        truncation=True, \n",
        "        padding=\"max_length\", \n",
        "        max_length=maxLen, \n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    logits = model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    labels = (probs >= threshold).astype(int)  # 1=positive, 0=negative\n",
        "    \n",
        "    return labels, probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Example Predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Inference on Custom Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model on some example texts\n",
        "test_texts = [\n",
        "    \"Stock prices are soaring! Great returns expected.\",\n",
        "    \"Market crash imminent, investors panic selling.\",\n",
        "    \"Company reports record profits and strong growth.\",\n",
        "    \"Bankruptcy fears as debt levels continue to rise.\"\n",
        "]\n",
        "\n",
        "labels, probs = predictTexts(test_texts, tokenizer, model, device=device)\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "print(\"=\"*80)\n",
        "for text, label, prob in zip(test_texts, labels, probs):\n",
        "    sentiment = \"POSITIVE\" if label == 1 else \"NEGATIVE\"\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Prediction: {sentiment} (confidence: {prob:.3f})\")\n",
        "    print(\"-\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
