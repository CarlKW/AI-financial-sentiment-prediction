{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Financial Sentiment Analysis - ALBERT Optimized\n",
        "\n",
        "This notebook uses ALBERT-base-v2 (12M parameters) with optimizations:\n",
        "- Early stopping\n",
        "- Learning rate scheduling\n",
        "- Higher dropout (0.3)\n",
        "- Best model checkpointing\n",
        "- Full model fine-tuning (safe with 12M params on 4K samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from pathlib import Path\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean tweet text by removing URLs, mentions, and extra whitespace\"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)      # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)         # Remove mentions\n",
        "    text = re.sub(r\"^user:\\s*\", \"\", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\"^user\\s*\", \"\", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\"[\\\"]+\", \"\", text)        # Remove quotes\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # Normalize whitespace\n",
        "    return text\n",
        "\n",
        "def labels_zero_one(y: int) -> int:\n",
        "    \"\"\"Convert {-1, 1} labels to {0, 1} for binary classification\"\"\"\n",
        "    return 1 if int(y) == 1 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = pd.read_csv(\"data/stock_data.csv\")\n",
        "df[\"Text\"] = df[\"Text\"].astype(str).apply(clean_text)\n",
        "\n",
        "print(f\"Dataset size: {len(df)} samples\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df[\"Sentiment\"].value_counts())\n",
        "print(f\"\\nSample texts:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinancialSentimentDataset(Dataset):\n",
        "    \"\"\"Dataset for financial sentiment classification\"\"\"\n",
        "    \n",
        "    def __init__(self, texts, labels, tokenizer, max_len: int = 128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBackbone(nn.Module):\n",
        "    \"\"\"Generic transformer encoder backbone\"\"\"\n",
        "    \n",
        "    def __init__(self, modelName: str = \"albert-base-v2\"):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(modelName)\n",
        "        self.hiddenSize = self.encoder.config.hidden_size\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        cls = out.last_hidden_state[:, 0]  # [CLS] token representation\n",
        "        return cls\n",
        "\n",
        "\n",
        "class BinaryHead(nn.Module):\n",
        "    \"\"\"Binary classification head with dropout regularization\"\"\"\n",
        "    \n",
        "    def __init__(self, inFeatures: int, pDrop: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(pDrop)\n",
        "        self.fc = nn.Linear(inFeatures, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x).squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "    \"\"\"Complete sentiment classification model\"\"\"\n",
        "    \n",
        "    def __init__(self, modelName: str = \"albert-base-v2\", pDrop: float = 0.3):\n",
        "        super().__init__()\n",
        "        self.backbone = TransformerBackbone(modelName)\n",
        "        self.head = BinaryHead(self.backbone.hiddenSize, pDrop)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        feats = self.backbone(input_ids, attention_mask)\n",
        "        logits = self.head(feats)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def step(model, batch, device, optimizer=None, train=True, clip=1.0):\n",
        "    \"\"\"Single training/validation step\"\"\"\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"label\"].float().to(device)\n",
        "\n",
        "    logits = model(input_ids, attention_mask)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "\n",
        "    if train:\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    y_true = labels.detach().cpu().numpy().astype(int)\n",
        "    return loss.item(), preds, y_true\n",
        "\n",
        "\n",
        "def runEpoch(model, loader, device, optimizer=None, scheduler=None, train=True):\n",
        "    \"\"\"Run one epoch of training or validation\"\"\"\n",
        "    model.train(train)\n",
        "    losses, allPreds, allTrue = [], [], []\n",
        "    \n",
        "    for batch in loader:\n",
        "        loss, preds, y_true = step(model, batch, device, optimizer, train)\n",
        "        losses.append(loss)\n",
        "        allPreds.extend(preds.tolist())\n",
        "        allTrue.extend(y_true.tolist())\n",
        "        \n",
        "        if train and scheduler is not None:\n",
        "            scheduler.step()\n",
        "    \n",
        "    avg_loss = float(sum(losses) / max(1, len(losses)))\n",
        "    acc = accuracy_score(allTrue, allPreds)\n",
        "    f1 = f1_score(allTrue, allPreds)\n",
        "    \n",
        "    return avg_loss, acc, f1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"albert-base-v2\"  # 12M parameters - perfect for ~4K samples\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 32  # Smaller batch for better generalization\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "DROPOUT = 0.3  # Higher dropout for regularization\n",
        "MAX_EPOCHS = 20\n",
        "PATIENCE = 3  # Early stopping patience\n",
        "WARMUP_STEPS = 100\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Prepare data splits\n",
        "labels_binary = df[\"Sentiment\"].apply(labels_zero_one).values\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"Text\"].values, \n",
        "    labels_binary, \n",
        "    test_size=0.2, \n",
        "    random_state=67,\n",
        "    stratify=labels_binary  # Stratified split for balanced classes\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Validation samples: {len(val_texts)}\")\n",
        "print(f\"\\nTraining class distribution: {np.bincount(train_labels)}\")\n",
        "print(f\"Validation class distribution: {np.bincount(val_labels)}\")\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = FinancialSentimentDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
        "val_dataset = FinancialSentimentDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"\\nBatches per epoch: {len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = SentimentClassifier(MODEL_NAME, pDrop=DROPOUT).to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Data-to-parameter ratio: {len(train_texts) / trainable_params:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training with Optimizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "total_steps = len(train_loader) * MAX_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=WARMUP_STEPS,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(f\"Total training steps: {total_steps}\")\n",
        "print(f\"Warmup steps: {WARMUP_STEPS}\")\n",
        "print(f\"\\nStarting training...\\n\")\n",
        "\n",
        "# Training loop with early stopping\n",
        "best_val_loss = float('inf')\n",
        "best_val_f1 = 0.0\n",
        "patience_counter = 0\n",
        "history = {'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
        "           'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "for epoch in range(MAX_EPOCHS):\n",
        "    # Training\n",
        "    train_loss, train_acc, train_f1 = runEpoch(\n",
        "        model, train_loader, device, optimizer, scheduler, train=True\n",
        "    )\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc, val_f1 = runEpoch(\n",
        "        model, val_loader, device, train=False\n",
        "    )\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_f1'].append(train_f1)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_f1'].append(val_f1)\n",
        "    \n",
        "    # Print progress\n",
        "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS}:\")\n",
        "    print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.3f}, F1: {train_f1:.3f}\")\n",
        "    print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.3f}, F1: {val_f1:.3f}\")\n",
        "    \n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_f1 = val_f1\n",
        "        patience_counter = 0\n",
        "        \n",
        "        # Save best model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'val_f1': val_f1,\n",
        "        }, \"albert_sentiment_best.pt\")\n",
        "        print(f\"  ✓ New best model saved! (Val Loss: {val_loss:.4f}, Val F1: {val_f1:.3f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"  No improvement ({patience_counter}/{PATIENCE})\")\n",
        "    \n",
        "    print()\n",
        "    \n",
        "    # Early stopping\n",
        "    if patience_counter >= PATIENCE:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "        print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "        print(f\"Best validation F1: {best_val_f1:.3f}\")\n",
        "        break\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "print(f\"Best model saved to: albert_sentiment_best.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training History Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
        "axes[0].plot(history['val_loss'], label='Validation', marker='s')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss over Training')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(history['train_acc'], label='Train', marker='o')\n",
        "axes[1].plot(history['val_acc'], label='Validation', marker='s')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Accuracy over Training')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# F1 plot\n",
        "axes[2].plot(history['train_f1'], label='Train', marker='o')\n",
        "axes[2].plot(history['val_f1'], label='Validation', marker='s')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('F1 Score')\n",
        "axes[2].set_title('F1 Score over Training')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Training curves saved to: training_history.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Best Model for Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best checkpoint\n",
        "checkpoint = torch.load(\"albert_sentiment_best.pt\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "print(f\"Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
        "print(f\"Validation F1: {checkpoint['val_f1']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_sentiment(texts, tokenizer, model, max_len=128, threshold=0.5, device=None):\n",
        "    \"\"\"Predict sentiment for input texts\n",
        "    \n",
        "    Args:\n",
        "        texts: Single string or list of strings\n",
        "        tokenizer: Tokenizer instance\n",
        "        model: Trained model\n",
        "        max_len: Maximum sequence length\n",
        "        threshold: Classification threshold (default 0.5)\n",
        "        device: Device to run inference on\n",
        "    \n",
        "    Returns:\n",
        "        labels: Binary labels (1=positive, 0=negative)\n",
        "        probs: Prediction probabilities\n",
        "    \"\"\"\n",
        "    device = device or next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    \n",
        "    # Clean texts\n",
        "    texts = [clean_text(t) for t in texts]\n",
        "    \n",
        "    enc = tokenizer(\n",
        "        texts, \n",
        "        truncation=True, \n",
        "        padding=\"max_length\", \n",
        "        max_length=max_len, \n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    logits = model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    labels = (probs >= threshold).astype(int)\n",
        "    \n",
        "    return labels, probs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test examples\n",
        "test_texts = [\n",
        "    \"Stock prices are soaring! Great quarterly earnings reported.\",\n",
        "    \"Company faces bankruptcy, stocks plummet.\",\n",
        "    \"Market remains stable with moderate growth.\",\n",
        "    \"$AAPL breaking new records today! 🚀\",\n",
        "    \"Massive losses reported in Q3, investors worried.\"\n",
        "]\n",
        "\n",
        "labels, probs = predict_sentiment(test_texts, tokenizer, model, device=device)\n",
        "\n",
        "print(\"\\nTest Predictions:\\n\" + \"=\"*80)\n",
        "for text, label, prob in zip(test_texts, labels, probs):\n",
        "    sentiment = \"POSITIVE\" if label == 1 else \"NEGATIVE\"\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {sentiment} (confidence: {prob:.3f})\")\n",
        "    print(\"-\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Architecture: ALBERT-base-v2 + Binary Classification Head\")\n",
        "print(f\"Total Parameters: {total_params:,}\")\n",
        "print(f\"Training Samples: {len(train_texts):,}\")\n",
        "print(f\"Validation Samples: {len(val_texts):,}\")\n",
        "print(f\"\\nHyperparameters:\")\n",
        "print(f\"  - Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  - Dropout: {DROPOUT}\")\n",
        "print(f\"  - Weight Decay: {WEIGHT_DECAY}\")\n",
        "print(f\"  - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  - Max Sequence Length: {MAX_LEN}\")\n",
        "print(f\"\\nOptimizations:\")\n",
        "print(f\"  ✓ Early stopping (patience={PATIENCE})\")\n",
        "print(f\"  ✓ Learning rate scheduling (warmup={WARMUP_STEPS} steps)\")\n",
        "print(f\"  ✓ Higher dropout for regularization\")\n",
        "print(f\"  ✓ Gradient clipping\")\n",
        "print(f\"  ✓ Weight decay (L2 regularization)\")\n",
        "print(f\"  ✓ Stratified train/val split\")\n",
        "print(f\"\\nBest Performance:\")\n",
        "print(f\"  - Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
        "print(f\"  - Validation F1: {checkpoint['val_f1']:.3f}\")\n",
        "print(f\"  - Epoch: {checkpoint['epoch']+1}\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
