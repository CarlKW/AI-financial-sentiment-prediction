{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unified Financial Text Analysis\n",
        "\n",
        "This notebook combines two trained models for comprehensive financial text analysis:\n",
        "1. **Stock Ticker Classification**: Identifies which stocks are mentioned in the text\n",
        "2. **Sentiment Analysis**: Determines if the sentiment is positive or negative\n",
        "\n",
        "## Models Used:\n",
        "- **Stock Classifier**: BERTweet-base model (checkpoint-5017) for multi-label stock classification\n",
        "- **Sentiment Analyzer**: FinBERT model (finbert_finetuned.pt) for binary sentiment classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Model Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FinBERT model class for sentiment analysis\n",
        "class FinbertBackbone(nn.Module):\n",
        "    def __init__(self, modelName: str = \"ProsusAI/finbert\"):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(modelName)\n",
        "        self.hiddenSize = self.encoder.config.hidden_size  # 768 for BERT-base\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        cls = out.last_hidden_state[:, 0]  # [CLS] token\n",
        "        return cls  # [batch, hidden]\n",
        "    \n",
        "class BinaryHead(nn.Module):\n",
        "    def __init__(self, inFeatures: int, pDrop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(pDrop)\n",
        "        self.fc = nn.Linear(inFeatures, 1)  # single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x).squeeze(-1)    # [batch]\n",
        "        return logits\n",
        "\n",
        "class FinbertBinaryClf(nn.Module):\n",
        "    def __init__(self, modelName: str = \"ProsusAI/finbert\", pDrop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.backbone = FinbertBackbone(modelName)\n",
        "        self.head = BinaryHead(self.backbone.hiddenSize, pDrop)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        feats = self.backbone(input_ids, attention_mask)\n",
        "        logits = self.head(feats)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text for sentiment analysis\"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)      \n",
        "    text = re.sub(r\"@\\w+\", \"\", text)        \n",
        "    text = re.sub(r\"^user:\\s*\", \"\", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\"^user\\s*\", \"\", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\"[\\\"]+\", \"\", text)        \n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
        "    return text\n",
        "\n",
        "def lables_zero_one(y: int) -> int:\n",
        "    \"\"\"Convert sentiment labels from {-1, 1} to {0, 1}\"\"\"\n",
        "    return 1 if int(y) == 1 else 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Models and Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading stock classification model...\n",
            "Stock tickers: ['AAPL', 'AMD', 'AMZN', 'BA', 'COST', 'DIS', 'GOOG', 'KO', 'META', 'MSFT', 'NFLX', 'NIO', 'Other', 'PG', 'PYPL', 'TSLA']\n",
            "Loading sentiment analysis model...\n",
            "✓ Both models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load stock classification model and tokenizer\n",
        "print(\"Loading stock classification model...\")\n",
        "stock_model_path = './results/checkpoint-5017/checkpoint-5017'  # Fixed path to nested directory\n",
        "stock_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
        "stock_model = AutoModelForSequenceClassification.from_pretrained(stock_model_path)\n",
        "stock_model.to(device)\n",
        "stock_model.eval()\n",
        "\n",
        "# Stock ticker labels (in order)\n",
        "stock_tickers = ['AAPL', 'AMD', 'AMZN', 'BA', 'COST', 'DIS', 'GOOG', 'KO', 'META', 'MSFT', 'NFLX', 'NIO', 'Other', 'PG', 'PYPL', 'TSLA']\n",
        "print(f\"Stock tickers: {stock_tickers}\")\n",
        "\n",
        "# Load sentiment analysis model\n",
        "print(\"Loading sentiment analysis model...\")\n",
        "sentiment_model = FinbertBinaryClf(\"ProsusAI/finbert\", pDrop=0.1)\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "# Load the trained weights\n",
        "checkpoint = torch.load('finbert_finetuned.pt', map_location=device)\n",
        "sentiment_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "sentiment_model.to(device)\n",
        "sentiment_model.eval()\n",
        "\n",
        "print(\"✓ Both models loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Individual Model Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_stocks(tweet_text, model, tokenizer, threshold=0.5):\n",
        "    \"\"\"Predict which stocks are mentioned in a tweet\"\"\"\n",
        "    \n",
        "    # Get device from model\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(tweet_text, return_tensors='pt', truncation=True, max_length=128)\n",
        "    \n",
        "    # Move inputs to same device as model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Get probabilities\n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits)[0]\n",
        "    \n",
        "    # Move back to CPU for numpy conversion\n",
        "    probs = probs.cpu()\n",
        "    \n",
        "    # Apply threshold\n",
        "    predictions = (probs > threshold).int().numpy()\n",
        "    \n",
        "    # Get predicted stock names\n",
        "    predicted_stocks = [stock_tickers[i] for i, pred in enumerate(predictions) if pred == 1]\n",
        "    \n",
        "    return predicted_stocks, probs.numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_sentiment(text, model, tokenizer, max_len=128, threshold=0.5, confidence_threshold=0.7):\n",
        "    \"\"\"Predict sentiment for input text\"\"\"\n",
        "    \n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    # Clean text\n",
        "    text = clean_text(text)\n",
        "    \n",
        "    # Tokenize\n",
        "    enc = tokenizer(\n",
        "        text, \n",
        "        truncation=True, \n",
        "        padding=\"max_length\", \n",
        "        max_length=max_len, \n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    # Get prediction\n",
        "    logits = model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
        "    prob = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "    \n",
        "    # Convert to binary prediction with confidence check\n",
        "    if prob >= threshold:\n",
        "        label = 1\n",
        "        confidence = prob\n",
        "    else:\n",
        "        label = 0\n",
        "        confidence = 1 - prob\n",
        "    \n",
        "    # Check if confidence is high enough\n",
        "    if confidence < confidence_threshold:\n",
        "        label = -1  # -1 represents \"unsure\"\n",
        "    \n",
        "    return label, prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Unified Analysis Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_financial_text(text):\n",
        "    \"\"\"\n",
        "    Unified function to analyze financial text for both stock mentions and sentiment\n",
        "    \n",
        "    Args:\n",
        "        text (str): Input financial text\n",
        "    \n",
        "    Returns:\n",
        "        dict: Analysis results containing:\n",
        "            - stocks: List of predicted stock tickers\n",
        "            - sentiment: 'positive', 'negative', or 'unsure'\n",
        "            - sentiment_confidence: Confidence score for sentiment\n",
        "            - stock_probabilities: Probability scores for each stock\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get stock predictions\n",
        "    predicted_stocks, stock_probs = predict_stocks(\n",
        "        text, stock_model, stock_tokenizer, threshold=0.7\n",
        "    )\n",
        "    \n",
        "    # Get sentiment prediction\n",
        "    sentiment_label, sentiment_prob = predict_sentiment(\n",
        "        text, sentiment_model, sentiment_tokenizer, threshold=0.5, confidence_threshold=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert sentiment to readable format\n",
        "    if sentiment_label == 1:\n",
        "        sentiment = 'positive'\n",
        "        sentiment_confidence = sentiment_prob\n",
        "    elif sentiment_label == 0:\n",
        "        sentiment = 'negative'\n",
        "        sentiment_confidence = 1 - sentiment_prob\n",
        "    else:  # sentiment_label == -1 (unsure)\n",
        "        sentiment = 'unsure'\n",
        "        sentiment_confidence = max(sentiment_prob, 1 - sentiment_prob)\n",
        "    \n",
        "    # Create stock probability dictionary\n",
        "    stock_probabilities = {ticker: float(prob) for ticker, prob in zip(stock_tickers, stock_probs)}\n",
        "    \n",
        "    return {\n",
        "        'stocks': predicted_stocks,\n",
        "        'sentiment': sentiment,\n",
        "        'sentiment_confidence': float(sentiment_confidence),\n",
        "        'stock_probabilities': stock_probabilities\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Example Usage and Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINANCIAL TEXT ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Text: Tesla and Apple crushing it! $TSLA $AAPL 🚀\n",
            "Predicted Stocks: ['AAPL']\n",
            "Sentiment: UNSURE (confidence: 0.661)\n",
            "Top Stock Probabilities: ['AAPL', 'TSLA']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Text: Microsoft stock is plummeting, terrible earnings report\n",
            "Predicted Stocks: []\n",
            "Sentiment: NEGATIVE (confidence: 0.920)\n",
            "Top Stock Probabilities: ['Unsure']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Text: Amazon and Google both showing strong growth this quarter\n",
            "Predicted Stocks: ['AMZN']\n",
            "Sentiment: POSITIVE (confidence: 0.940)\n",
            "Top Stock Probabilities: ['AMZN']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 4:\n",
            "Text: Netflix subscription numbers are declining rapidly\n",
            "Predicted Stocks: ['NFLX']\n",
            "Sentiment: NEGATIVE (confidence: 0.914)\n",
            "Top Stock Probabilities: ['NFLX']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 5:\n",
            "Text: Meta's new VR headset is revolutionary! $META\n",
            "Predicted Stocks: ['META']\n",
            "Sentiment: POSITIVE (confidence: 0.940)\n",
            "Top Stock Probabilities: ['META']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 6:\n",
            "Text: Disney's streaming service is struggling with competition\n",
            "Predicted Stocks: ['AAPL']\n",
            "Sentiment: NEGATIVE (confidence: 0.892)\n",
            "Top Stock Probabilities: ['AAPL']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 7:\n",
            "Text: NVIDIA and AMD GPUs are in high demand for AI workloads\n",
            "Predicted Stocks: ['AMD']\n",
            "Sentiment: POSITIVE (confidence: 0.922)\n",
            "Top Stock Probabilities: ['AMD']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 8:\n",
            "Text: PayPal's new features are disappointing users\n",
            "Predicted Stocks: []\n",
            "Sentiment: NEGATIVE (confidence: 0.914)\n",
            "Top Stock Probabilities: ['Unsure']\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test examples\n",
        "test_texts = [\n",
        "    \"Tesla and Apple crushing it! $TSLA $AAPL 🚀\",\n",
        "    \"Microsoft stock is plummeting, terrible earnings report\",\n",
        "    \"Amazon and Google both showing strong growth this quarter\",\n",
        "    \"Netflix subscription numbers are declining rapidly\",\n",
        "    \"Meta's new VR headset is revolutionary! $META\",\n",
        "    \"Disney's streaming service is struggling with competition\",\n",
        "    \"NVIDIA and AMD GPUs are in high demand for AI workloads\",\n",
        "    \"PayPal's new features are disappointing users\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINANCIAL TEXT ANALYSIS RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    \n",
        "    # Analyze the text\n",
        "    result = analyze_financial_text(text)\n",
        "    \n",
        "    print(f\"Predicted Stocks: {result['stocks']}\")\n",
        "    print(f\"Sentiment: {result['sentiment'].upper()} (confidence: {result['sentiment_confidence']:.3f})\")\n",
        "    \n",
        "    # Show top stock probabilities\n",
        "    top_stocks = [stock for stock, prob in result['stock_probabilities'].items() if prob > 0.5]\n",
        "    if top_stocks == []:\n",
        "        top_stocks = [\"Unsure\"]\n",
        "    #sorted(result['stock_probabilities'].items(), \n",
        "                      #key=lambda x: x[1], reverse=True)[:3]\n",
        "    print(f\"Top Stock Probabilities: {top_stocks}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Interactive Analysis Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_analysis():\n",
        "    \"\"\"Interactive function for analyzing custom text\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INTERACTIVE FINANCIAL TEXT ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Enter financial text to analyze (type 'quit' to exit):\")\n",
        "    \n",
        "    while True:\n",
        "        text = input(\"\\nEnter text: \").strip()\n",
        "        \n",
        "        if text.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "            \n",
        "        if not text:\n",
        "            print(\"Please enter some text.\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            result = analyze_financial_text(text)\n",
        "            \n",
        "            print(f\"\\n📊 ANALYSIS RESULTS:\")\n",
        "            print(f\"📈 Stocks: {', '.join(result['stocks']) if result['stocks'] else 'None detected'}\")\n",
        "            print(f\"😊 Sentiment: {result['sentiment'].upper()} ({result['sentiment_confidence']:.1%} confidence)\")\n",
        "            \n",
        "            # Show top 3 stock probabilities\n",
        "            top_stocks = sorted(result['stock_probabilities'].items(), \n",
        "                              key=lambda x: x[1], reverse=True)[:3]\n",
        "            print(f\"🎯 Top Stock Probabilities:\")\n",
        "            for stock, prob in top_stocks:\n",
        "                print(f\"   {stock}: {prob:.1%}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing text: {e}\")\n",
        "\n",
        "# Uncomment the line below to run interactive analysis\n",
        "# interactive_analysis()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Batch Analysis Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BATCH ANALYSIS EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Text 1: Apple's new iPhone sales are breaking records!\n",
            "  Stocks: ['AAPL']\n",
            "  Sentiment: positive (95.4%)\n",
            "\n",
            "Text 2: Tesla stock is crashing after the recall news\n",
            "  Stocks: ['TSLA']\n",
            "  Sentiment: negative (88.9%)\n",
            "\n",
            "Text 3: Microsoft Azure cloud revenue is growing rapidly\n",
            "  Stocks: []\n",
            "  Sentiment: positive (93.6%)\n"
          ]
        }
      ],
      "source": [
        "def batch_analyze(texts):\n",
        "    \"\"\"\n",
        "    Analyze multiple texts in batch\n",
        "    \n",
        "    Args:\n",
        "        texts (list): List of financial texts to analyze\n",
        "    \n",
        "    Returns:\n",
        "        list: List of analysis results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for text in texts:\n",
        "        result = analyze_financial_text(text)\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'stocks': result['stocks'],\n",
        "            'sentiment': result['sentiment'],\n",
        "            'sentiment_confidence': result['sentiment_confidence']\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example batch analysis\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BATCH ANALYSIS EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "batch_texts = [\n",
        "    \"Apple's new iPhone sales are breaking records!\",\n",
        "    \"Tesla stock is crashing after the recall news\",\n",
        "    \"Microsoft Azure cloud revenue is growing rapidly\"\n",
        "]\n",
        "\n",
        "batch_results = batch_analyze(batch_texts)\n",
        "\n",
        "for i, result in enumerate(batch_results, 1):\n",
        "    print(f\"\\nText {i}: {result['text']}\")\n",
        "    print(f\"  Stocks: {result['stocks']}\")\n",
        "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_confidence']:.1%})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Model Performance Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Sentiment-Stock Price Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def analyze_sentiment_stock_correlation(target_date, combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyze correlation between tweet sentiment and stock price movements for a given date\n",
        "    \n",
        "    Args:\n",
        "        target_date (str): Date in format 'YYYY-MM-DD' (e.g., '2022-09-29')\n",
        "        combined_data_path (str): Path to combined tweets and stock data CSV file\n",
        "    \n",
        "    Returns:\n",
        "        dict: Analysis results with sentiment summary and price correlation\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"🔍 Analyzing sentiment-stock correlation for {target_date}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load combined dataset\n",
        "    print(\"📊 Loading combined dataset...\")\n",
        "    df = pd.read_csv(combined_data_path)\n",
        "    \n",
        "    # Convert date column\n",
        "    df['date_only'] = pd.to_datetime(df['date_only']).dt.date\n",
        "    target_date_obj = pd.to_datetime(target_date).date()\n",
        "    \n",
        "    # Filter data for target date\n",
        "    day_data = df[df['date_only'] == target_date_obj]\n",
        "    \n",
        "    print(f\"📈 Found {len(day_data)} tweet-stock records for {target_date}\")\n",
        "    \n",
        "    if len(day_data) == 0:\n",
        "        return {\"error\": f\"No data found for {target_date}\"}\n",
        "    \n",
        "    # Analyze tweets for sentiment and stock mentions\n",
        "    print(\"🤖 Analyzing tweets with AI models...\")\n",
        "    \n",
        "    # Store results by stock\n",
        "    stock_analysis = defaultdict(lambda: {\n",
        "        'tweets': [],\n",
        "        'sentiments': [],\n",
        "        'sentiment_confidences': [],\n",
        "        'positive_count': 0,\n",
        "        'negative_count': 0,\n",
        "        'unsure_count': 0,\n",
        "        'total_tweets': 0,\n",
        "        'daily_return': None,\n",
        "        'price_data': None\n",
        "    })\n",
        "    \n",
        "    # Process each tweet-stock combination\n",
        "    for idx, row in day_data.iterrows():\n",
        "        tweet_text = row['Tweet']\n",
        "        stock_name = row['Stock Name']\n",
        "        daily_return = row['daily_return']\n",
        "        \n",
        "        # Get price data for this stock\n",
        "        price_data = {\n",
        "            'open': row['Open'],\n",
        "            'close': row['Close'],\n",
        "            'high': row['High'],\n",
        "            'low': row['Low'],\n",
        "            'volume': row['Volume'],\n",
        "            'daily_return': daily_return\n",
        "        }\n",
        "        \n",
        "        # Analyze with our models\n",
        "        try:\n",
        "            result = analyze_financial_text(tweet_text)\n",
        "            \n",
        "            # Store tweet info\n",
        "            tweet_info = {\n",
        "                'text': tweet_text,\n",
        "                'sentiment': result['sentiment'],\n",
        "                'confidence': result['sentiment_confidence'],\n",
        "                'mentioned_stocks': result['stocks']\n",
        "            }\n",
        "            \n",
        "            # Update analysis for this specific stock\n",
        "            if stock_name in result['stocks'] or stock_name in [s for s in result['stocks']]:\n",
        "                stock_analysis[stock_name]['tweets'].append(tweet_info)\n",
        "                stock_analysis[stock_name]['sentiments'].append(result['sentiment'])\n",
        "                stock_analysis[stock_name]['sentiment_confidences'].append(result['sentiment_confidence'])\n",
        "                stock_analysis[stock_name]['total_tweets'] += 1\n",
        "                stock_analysis[stock_name]['daily_return'] = daily_return\n",
        "                stock_analysis[stock_name]['price_data'] = price_data\n",
        "                \n",
        "                # Count sentiment types\n",
        "                if result['sentiment'] == 'positive':\n",
        "                    stock_analysis[stock_name]['positive_count'] += 1\n",
        "                elif result['sentiment'] == 'negative':\n",
        "                    stock_analysis[stock_name]['negative_count'] += 1\n",
        "                else:  # unsure\n",
        "                    stock_analysis[stock_name]['unsure_count'] += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error analyzing tweet: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Calculate sentiment summaries and correlations\n",
        "    print(\"\\n📊 SENTIMENT ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    results = {\n",
        "        'date': target_date,\n",
        "        'total_records_analyzed': len(day_data),\n",
        "        'stocks_analyzed': {},\n",
        "        'price_correlations': {}\n",
        "    }\n",
        "    \n",
        "    for stock, analysis in stock_analysis.items():\n",
        "        if analysis['total_tweets'] == 0:\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n🏢 {stock} Analysis:\")\n",
        "        print(f\"   📝 Tweets mentioning {stock}: {analysis['total_tweets']}\")\n",
        "        print(f\"   😊 Positive: {analysis['positive_count']} ({analysis['positive_count']/analysis['total_tweets']*100:.1f}%)\")\n",
        "        print(f\"   😞 Negative: {analysis['negative_count']} ({analysis['negative_count']/analysis['total_tweets']*100:.1f}%)\")\n",
        "        print(f\"   ❓ Unsure: {analysis['unsure_count']} ({analysis['unsure_count']/analysis['total_tweets']*100:.1f}%)\")\n",
        "        \n",
        "        # Calculate overall sentiment\n",
        "        if analysis['positive_count'] > analysis['negative_count']:\n",
        "            overall_sentiment = 'positive'\n",
        "            sentiment_strength = analysis['positive_count'] / analysis['total_tweets']\n",
        "        elif analysis['negative_count'] > analysis['positive_count']:\n",
        "            overall_sentiment = 'negative'\n",
        "            sentiment_strength = analysis['negative_count'] / analysis['total_tweets']\n",
        "        else:\n",
        "            overall_sentiment = 'neutral'\n",
        "            sentiment_strength = 0.5\n",
        "        \n",
        "        print(f\"   🎯 Overall Sentiment: {overall_sentiment.upper()} (strength: {sentiment_strength:.2f})\")\n",
        "        \n",
        "        # Get stock price data\n",
        "        if analysis['price_data'] is not None:\n",
        "            price_data = analysis['price_data']\n",
        "            daily_return = analysis['daily_return']\n",
        "            daily_return_pct = daily_return * 100\n",
        "            \n",
        "            print(f\"   💰 Stock Price: ${price_data['open']:.2f} → ${price_data['close']:.2f}\")\n",
        "            print(f\"   📈 Daily Return: {daily_return_pct:+.2f}%\")\n",
        "            \n",
        "            # Determine price direction\n",
        "            if daily_return > 0:\n",
        "                price_direction = 'up'\n",
        "            elif daily_return < 0:\n",
        "                price_direction = 'down'\n",
        "            else:\n",
        "                price_direction = 'flat'\n",
        "            \n",
        "            # Check correlation\n",
        "            sentiment_price_match = (\n",
        "                (overall_sentiment == 'positive' and price_direction == 'up') or\n",
        "                (overall_sentiment == 'negative' and price_direction == 'down') or\n",
        "                (overall_sentiment == 'neutral' and price_direction == 'flat')\n",
        "            )\n",
        "            \n",
        "            correlation_status = \"✅ MATCH\" if sentiment_price_match else \"❌ NO MATCH\"\n",
        "            print(f\"   🔗 Correlation: {correlation_status}\")\n",
        "            \n",
        "            # Store results\n",
        "            results['stocks_analyzed'][stock] = {\n",
        "                'tweet_count': analysis['total_tweets'],\n",
        "                'sentiment_breakdown': {\n",
        "                    'positive': analysis['positive_count'],\n",
        "                    'negative': analysis['negative_count'],\n",
        "                    'unsure': analysis['unsure_count']\n",
        "                },\n",
        "                'overall_sentiment': overall_sentiment,\n",
        "                'sentiment_strength': sentiment_strength,\n",
        "                'price_data': {\n",
        "                    'open': float(price_data['open']),\n",
        "                    'close': float(price_data['close']),\n",
        "                    'high': float(price_data['high']),\n",
        "                    'low': float(price_data['low']),\n",
        "                    'volume': int(price_data['volume']),\n",
        "                    'daily_return': float(daily_return),\n",
        "                    'daily_return_pct': float(daily_return_pct),\n",
        "                    'direction': price_direction\n",
        "                },\n",
        "                'correlation_match': sentiment_price_match\n",
        "            }\n",
        "            \n",
        "            results['price_correlations'][stock] = {\n",
        "                'sentiment': overall_sentiment,\n",
        "                'price_direction': price_direction,\n",
        "                'match': sentiment_price_match\n",
        "            }\n",
        "        else:\n",
        "            print(f\"   ⚠️ No price data found for {stock}\")\n",
        "            results['stocks_analyzed'][stock] = {\n",
        "                'tweet_count': analysis['total_tweets'],\n",
        "                'sentiment_breakdown': {\n",
        "                    'positive': analysis['positive_count'],\n",
        "                    'negative': analysis['negative_count'],\n",
        "                    'unsure': analysis['unsure_count']\n",
        "                },\n",
        "                'overall_sentiment': overall_sentiment,\n",
        "                'sentiment_strength': sentiment_strength,\n",
        "                'price_data': None,\n",
        "                'correlation_match': None\n",
        "            }\n",
        "    \n",
        "    # Calculate overall correlation summary\n",
        "    print(f\"\\n📊 CORRELATION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    total_stocks = len([s for s in results['stocks_analyzed'].keys() if results['stocks_analyzed'][s]['price_data'] is not None])\n",
        "    matches = sum(1 for s in results['price_correlations'].values() if s['match'])\n",
        "    \n",
        "    if total_stocks > 0:\n",
        "        correlation_rate = matches / total_stocks * 100\n",
        "        print(f\"🎯 Overall Correlation Rate: {correlation_rate:.1f}% ({matches}/{total_stocks} stocks)\")\n",
        "        \n",
        "        if correlation_rate >= 70:\n",
        "            print(\"✅ Strong correlation between sentiment and stock prices!\")\n",
        "        elif correlation_rate >= 50:\n",
        "            print(\"⚠️ Moderate correlation between sentiment and stock prices\")\n",
        "        else:\n",
        "            print(\"❌ Weak correlation between sentiment and stock prices\")\n",
        "    else:\n",
        "        print(\"⚠️ No stock price data available for correlation analysis\")\n",
        "    \n",
        "    results['correlation_summary'] = {\n",
        "        'total_stocks_with_prices': total_stocks,\n",
        "        'matches': matches,\n",
        "        'correlation_rate': correlation_rate if total_stocks > 0 else 0\n",
        "    }\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_multiple_dates(date_list, combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyze sentiment-stock correlation for multiple dates\n",
        "    \n",
        "    Args:\n",
        "        date_list (list): List of dates in format 'YYYY-MM-DD'\n",
        "        combined_data_path (str): Path to combined tweets and stock data CSV file\n",
        "    \n",
        "    Returns:\n",
        "        dict: Combined analysis results for all dates\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"📅 Analyzing {len(date_list)} dates for sentiment-stock correlation\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    all_results = {}\n",
        "    total_correlations = []\n",
        "    \n",
        "    for date in date_list:\n",
        "        print(f\"\\n🔍 Analyzing {date}...\")\n",
        "        result = analyze_sentiment_stock_correlation(date, combined_data_path)\n",
        "        all_results[date] = result\n",
        "        \n",
        "        if 'correlation_summary' in result:\n",
        "            total_correlations.append(result['correlation_summary']['correlation_rate'])\n",
        "    \n",
        "    # Calculate overall statistics\n",
        "    if total_correlations:\n",
        "        avg_correlation = np.mean(total_correlations)\n",
        "        print(f\"\\n📊 OVERALL ANALYSIS SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"📅 Dates analyzed: {len(date_list)}\")\n",
        "        print(f\"📈 Average correlation rate: {avg_correlation:.1f}%\")\n",
        "        print(f\"📊 Correlation range: {min(total_correlations):.1f}% - {max(total_correlations):.1f}%\")\n",
        "        \n",
        "        if avg_correlation >= 70:\n",
        "            print(\"✅ Strong overall correlation between sentiment and stock prices!\")\n",
        "        elif avg_correlation >= 50:\n",
        "            print(\"⚠️ Moderate overall correlation between sentiment and stock prices\")\n",
        "        else:\n",
        "            print(\"❌ Weak overall correlation between sentiment and stock prices\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def get_available_dates(combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "    \"\"\"\n",
        "    Get list of available dates in the combined dataset\n",
        "    \n",
        "    Returns:\n",
        "        dict: Available dates in the combined dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    df = pd.read_csv(combined_data_path)\n",
        "    df['date_only'] = pd.to_datetime(df['date_only']).dt.date\n",
        "    \n",
        "    available_dates = sorted(df['date_only'].unique())\n",
        "    \n",
        "    # Get some statistics\n",
        "    total_records = len(df)\n",
        "    unique_stocks = df['Stock Name'].nunique()\n",
        "    \n",
        "    return {\n",
        "        'available_dates': [d.strftime('%Y-%m-%d') for d in available_dates],\n",
        "        'total_dates': len(available_dates),\n",
        "        'total_records': total_records,\n",
        "        'unique_stocks': unique_stocks,\n",
        "        'date_range': {\n",
        "            'start': available_dates[0].strftime('%Y-%m-%d'),\n",
        "            'end': available_dates[-1].strftime('%Y-%m-%d')\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Example Usage: Sentiment-Stock Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📅 CHECKING AVAILABLE DATES\n",
            "============================================================\n",
            "📊 Total dates: 251\n",
            "📈 Total records: 74,846\n",
            "🏢 Unique stocks: 25\n",
            "📅 Date range: 2021-10-01 to 2022-09-29\n",
            "\n",
            "📅 Example available dates:\n",
            "   2021-10-01\n",
            "   2021-10-04\n",
            "   2021-10-05\n",
            "   2021-10-06\n",
            "   2021-10-07\n",
            "   ... and 246 more dates\n"
          ]
        }
      ],
      "source": [
        "# Check available dates in the combined dataset\n",
        "print(\"📅 CHECKING AVAILABLE DATES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "available_dates = get_available_dates()\n",
        "print(f\"📊 Total dates: {available_dates['total_dates']}\")\n",
        "print(f\"📈 Total records: {available_dates['total_records']:,}\")\n",
        "print(f\"🏢 Unique stocks: {available_dates['unique_stocks']}\")\n",
        "print(f\"📅 Date range: {available_dates['date_range']['start']} to {available_dates['date_range']['end']}\")\n",
        "\n",
        "# Show some example dates\n",
        "if available_dates['available_dates']:\n",
        "    print(f\"\\n📅 Example available dates:\")\n",
        "    for date in available_dates['available_dates'][:5]:\n",
        "        print(f\"   {date}\")\n",
        "    if len(available_dates['available_dates']) > 5:\n",
        "        print(f\"   ... and {len(available_dates['available_dates']) - 5} more dates\")\n",
        "else:\n",
        "    print(\"⚠️ No dates found in the combined dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 EXAMPLE ANALYSIS FOR 2021-10-01\n",
            "================================================================================\n",
            "🔍 Analyzing sentiment-stock correlation for 2021-10-01\n",
            "================================================================================\n",
            "📊 Loading combined dataset...\n",
            "📈 Found 214 tweet-stock records for 2021-10-01\n",
            "🤖 Analyzing tweets with AI models...\n",
            "\n",
            "📊 SENTIMENT ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "🏢 TSLA Analysis:\n",
            "   📝 Tweets mentioning TSLA: 84\n",
            "   😊 Positive: 61 (72.6%)\n",
            "   😞 Negative: 5 (6.0%)\n",
            "   ❓ Unsure: 18 (21.4%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.73)\n",
            "   💰 Stock Price: $259.47 → $258.41\n",
            "   📈 Daily Return: -0.03%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 MSFT Analysis:\n",
            "   📝 Tweets mentioning MSFT: 9\n",
            "   😊 Positive: 9 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $282.12 → $289.10\n",
            "   📈 Daily Return: +2.55%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 AMZN Analysis:\n",
            "   📝 Tweets mentioning AMZN: 16\n",
            "   😊 Positive: 16 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $164.45 → $164.16\n",
            "   📈 Daily Return: -0.05%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 GOOG Analysis:\n",
            "   📝 Tweets mentioning GOOG: 2\n",
            "   😊 Positive: 0 (0.0%)\n",
            "   😞 Negative: 1 (50.0%)\n",
            "   ❓ Unsure: 1 (50.0%)\n",
            "   🎯 Overall Sentiment: NEGATIVE (strength: 0.50)\n",
            "   💰 Stock Price: $133.55 → $136.46\n",
            "   📈 Daily Return: +2.40%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 AMD Analysis:\n",
            "   📝 Tweets mentioning AMD: 3\n",
            "   😊 Positive: 3 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $102.60 → $102.45\n",
            "   📈 Daily Return: -0.44%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 AAPL Analysis:\n",
            "   📝 Tweets mentioning AAPL: 20\n",
            "   😊 Positive: 18 (90.0%)\n",
            "   😞 Negative: 1 (5.0%)\n",
            "   ❓ Unsure: 1 (5.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.90)\n",
            "   💰 Stock Price: $141.90 → $142.65\n",
            "   📈 Daily Return: +0.81%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 NFLX Analysis:\n",
            "   📝 Tweets mentioning NFLX: 11\n",
            "   😊 Positive: 10 (90.9%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 1 (9.1%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.91)\n",
            "   💰 Stock Price: $604.24 → $613.15\n",
            "   📈 Daily Return: +0.46%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 DIS Analysis:\n",
            "   📝 Tweets mentioning DIS: 2\n",
            "   😊 Positive: 2 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $172.28 → $176.01\n",
            "   📈 Daily Return: +4.04%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 NIO Analysis:\n",
            "   📝 Tweets mentioning NIO: 18\n",
            "   😊 Positive: 15 (83.3%)\n",
            "   😞 Negative: 1 (5.6%)\n",
            "   ❓ Unsure: 2 (11.1%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.83)\n",
            "   💰 Stock Price: $36.63 → $35.38\n",
            "   📈 Daily Return: -0.70%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "📊 CORRELATION SUMMARY\n",
            "================================================================================\n",
            "🎯 Overall Correlation Rate: 44.4% (4/9 stocks)\n",
            "❌ Weak correlation between sentiment and stock prices\n",
            "\n",
            "📊 SUMMARY FOR 2021-10-01:\n",
            "   📝 Total records analyzed: 214\n",
            "   🏢 Stocks analyzed: 9\n",
            "   🎯 Correlation rate: 44.4%\n"
          ]
        }
      ],
      "source": [
        "# Example: Analyze a specific date\n",
        "if available_dates['available_dates']:\n",
        "    # Use the first available date as an example\n",
        "    example_date = available_dates['available_dates'][0]\n",
        "    print(f\"\\n🔍 EXAMPLE ANALYSIS FOR {example_date}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Run the analysis\n",
        "    result = analyze_sentiment_stock_correlation(example_date)\n",
        "    \n",
        "    if 'error' not in result:\n",
        "        print(f\"\\n📊 SUMMARY FOR {example_date}:\")\n",
        "        print(f\"   📝 Total records analyzed: {result['total_records_analyzed']}\")\n",
        "        print(f\"   🏢 Stocks analyzed: {len(result['stocks_analyzed'])}\")\n",
        "        if 'correlation_summary' in result:\n",
        "            print(f\"   🎯 Correlation rate: {result['correlation_summary']['correlation_rate']:.1f}%\")\n",
        "    else:\n",
        "        print(f\"❌ Error: {result['error']}\")\n",
        "else:\n",
        "    print(\"⚠️ No dates available for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📅 MULTIPLE DATE ANALYSIS EXAMPLE\n",
            "================================================================================\n",
            "Analyzing dates: ['2021-10-01', '2021-10-04', '2021-10-05']\n",
            "📅 Analyzing 3 dates for sentiment-stock correlation\n",
            "================================================================================\n",
            "\n",
            "🔍 Analyzing 2021-10-01...\n",
            "🔍 Analyzing sentiment-stock correlation for 2021-10-01\n",
            "================================================================================\n",
            "📊 Loading combined dataset...\n",
            "📈 Found 214 tweet-stock records for 2021-10-01\n",
            "🤖 Analyzing tweets with AI models...\n",
            "\n",
            "📊 SENTIMENT ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "🏢 TSLA Analysis:\n",
            "   📝 Tweets mentioning TSLA: 84\n",
            "   😊 Positive: 61 (72.6%)\n",
            "   😞 Negative: 5 (6.0%)\n",
            "   ❓ Unsure: 18 (21.4%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.73)\n",
            "   💰 Stock Price: $259.47 → $258.41\n",
            "   📈 Daily Return: -0.03%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 MSFT Analysis:\n",
            "   📝 Tweets mentioning MSFT: 9\n",
            "   😊 Positive: 9 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $282.12 → $289.10\n",
            "   📈 Daily Return: +2.55%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 AMZN Analysis:\n",
            "   📝 Tweets mentioning AMZN: 16\n",
            "   😊 Positive: 16 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $164.45 → $164.16\n",
            "   📈 Daily Return: -0.05%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 GOOG Analysis:\n",
            "   📝 Tweets mentioning GOOG: 2\n",
            "   😊 Positive: 0 (0.0%)\n",
            "   😞 Negative: 1 (50.0%)\n",
            "   ❓ Unsure: 1 (50.0%)\n",
            "   🎯 Overall Sentiment: NEGATIVE (strength: 0.50)\n",
            "   💰 Stock Price: $133.55 → $136.46\n",
            "   📈 Daily Return: +2.40%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 AMD Analysis:\n",
            "   📝 Tweets mentioning AMD: 3\n",
            "   😊 Positive: 3 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $102.60 → $102.45\n",
            "   📈 Daily Return: -0.44%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "🏢 AAPL Analysis:\n",
            "   📝 Tweets mentioning AAPL: 20\n",
            "   😊 Positive: 18 (90.0%)\n",
            "   😞 Negative: 1 (5.0%)\n",
            "   ❓ Unsure: 1 (5.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.90)\n",
            "   💰 Stock Price: $141.90 → $142.65\n",
            "   📈 Daily Return: +0.81%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 NFLX Analysis:\n",
            "   📝 Tweets mentioning NFLX: 11\n",
            "   😊 Positive: 10 (90.9%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 1 (9.1%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.91)\n",
            "   💰 Stock Price: $604.24 → $613.15\n",
            "   📈 Daily Return: +0.46%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 DIS Analysis:\n",
            "   📝 Tweets mentioning DIS: 2\n",
            "   😊 Positive: 2 (100.0%)\n",
            "   😞 Negative: 0 (0.0%)\n",
            "   ❓ Unsure: 0 (0.0%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 1.00)\n",
            "   💰 Stock Price: $172.28 → $176.01\n",
            "   📈 Daily Return: +4.04%\n",
            "   🔗 Correlation: ✅ MATCH\n",
            "\n",
            "🏢 NIO Analysis:\n",
            "   📝 Tweets mentioning NIO: 18\n",
            "   😊 Positive: 15 (83.3%)\n",
            "   😞 Negative: 1 (5.6%)\n",
            "   ❓ Unsure: 2 (11.1%)\n",
            "   🎯 Overall Sentiment: POSITIVE (strength: 0.83)\n",
            "   💰 Stock Price: $36.63 → $35.38\n",
            "   📈 Daily Return: -0.70%\n",
            "   🔗 Correlation: ❌ NO MATCH\n",
            "\n",
            "📊 CORRELATION SUMMARY\n",
            "================================================================================\n",
            "🎯 Overall Correlation Rate: 44.4% (4/9 stocks)\n",
            "❌ Weak correlation between sentiment and stock prices\n",
            "\n",
            "🔍 Analyzing 2021-10-04...\n",
            "🔍 Analyzing sentiment-stock correlation for 2021-10-04\n",
            "================================================================================\n",
            "📊 Loading combined dataset...\n",
            "📈 Found 330 tweet-stock records for 2021-10-04\n",
            "🤖 Analyzing tweets with AI models...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnalyzing dates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdates_to_analyze\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Run multi-date analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m multi_results = \u001b[43manalyze_multiple_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdates_to_analyze\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m📊 MULTI-DATE SUMMARY:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m date, result \u001b[38;5;129;01min\u001b[39;00m multi_results.items():\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36manalyze_multiple_dates\u001b[39m\u001b[34m(date_list, combined_data_path)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m date_list:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔍 Analyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     result = \u001b[43manalyze_sentiment_stock_correlation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     all_results[date] = result\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcorrelation_summary\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36manalyze_sentiment_stock_correlation\u001b[39m\u001b[34m(target_date, combined_data_path)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Analyze with our models\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     result = \u001b[43manalyze_financial_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# Store tweet info\u001b[39;00m\n\u001b[32m     74\u001b[39m     tweet_info = {\n\u001b[32m     75\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: tweet_text,\n\u001b[32m     76\u001b[39m         \u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m: result[\u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     77\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m: result[\u001b[33m'\u001b[39m\u001b[33msentiment_confidence\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     78\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmentioned_stocks\u001b[39m\u001b[33m'\u001b[39m: result[\u001b[33m'\u001b[39m\u001b[33mstocks\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     79\u001b[39m     }\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36manalyze_financial_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mUnified function to analyze financial text for both stock mentions and sentiment\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m        - stock_probabilities: Probability scores for each stock\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Get stock predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m predicted_stocks, stock_probs = \u001b[43mpredict_stocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Get sentiment prediction\u001b[39;00m\n\u001b[32m     22\u001b[39m sentiment_label, sentiment_prob = predict_sentiment(\n\u001b[32m     23\u001b[39m     text, sentiment_model, sentiment_tokenizer, threshold=\u001b[32m0.5\u001b[39m, confidence_threshold=\u001b[32m0.7\u001b[39m\n\u001b[32m     24\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mpredict_stocks\u001b[39m\u001b[34m(tweet_text, model, tokenizer, threshold)\u001b[39m\n\u001b[32m     12\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs.items()}\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Get probabilities\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m probs = torch.sigmoid(outputs.logits)[\u001b[32m0\u001b[39m]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Move back to CPU for numpy conversion\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1188\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1172\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1184\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1186\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1200\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:862\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    856\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    857\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    858\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    859\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    860\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    876\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:606\u001b[39m, in \u001b[36mRobertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    602\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m    604\u001b[39m layer_head_mask = head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    617\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:513\u001b[39m, in \u001b[36mRobertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    512\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:449\u001b[39m, in \u001b[36mRobertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_values, output_attentions, cache_position)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    431\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m     cache_position: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    439\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]:\n\u001b[32m    440\u001b[39m     self_outputs = \u001b[38;5;28mself\u001b[39m.self(\n\u001b[32m    441\u001b[39m         hidden_states,\n\u001b[32m    442\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m         cache_position=cache_position,\n\u001b[32m    448\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:387\u001b[39m, in \u001b[36mRobertaSelfOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    389\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlk\\OneDrive\\Skrivbord\\code\\Deep_nn_project\\AI-financial-sentiment-prediction\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Example: Analyze multiple dates\n",
        "if available_dates['available_dates'] and len(available_dates['available_dates']) >= 3:\n",
        "    print(f\"\\n📅 MULTIPLE DATE ANALYSIS EXAMPLE\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Analyze first 3 available dates\n",
        "    dates_to_analyze = available_dates['available_dates'][:3]\n",
        "    print(f\"Analyzing dates: {dates_to_analyze}\")\n",
        "    \n",
        "    # Run multi-date analysis\n",
        "    multi_results = analyze_multiple_dates(dates_to_analyze)\n",
        "    \n",
        "    print(f\"\\n📊 MULTI-DATE SUMMARY:\")\n",
        "    for date, result in multi_results.items():\n",
        "        if 'correlation_summary' in result:\n",
        "            print(f\"   {date}: {result['correlation_summary']['correlation_rate']:.1f}% correlation\")\n",
        "        else:\n",
        "            print(f\"   {date}: No data available\")\n",
        "else:\n",
        "    print(\"⚠️ Not enough dates available for multi-date analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Stock Correlation Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def plot_stock_correlation(stock_name, start_date=None, num_days=10, combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "    \"\"\"\n",
        "    Create a correlation plot for a specific stock over a date range\n",
        "    \n",
        "    Args:\n",
        "        stock_name (str): Stock ticker (e.g., 'TSLA', 'AAPL')\n",
        "        start_date (str): Start date in 'YYYY-MM-DD' format. If None, uses most recent dates\n",
        "        num_days (int): Number of days to analyze\n",
        "        combined_data_path (str): Path to combined dataset\n",
        "    \n",
        "    Returns:\n",
        "        dict: Analysis results with correlation data\n",
        "    \"\"\"\n",
        "    \n",
        "    # Load dataset\n",
        "    df = pd.read_csv(combined_data_path)\n",
        "    df['date_only'] = pd.to_datetime(df['date_only']).dt.date\n",
        "    \n",
        "    # Get all available dates (not filtered by stock yet)\n",
        "    all_dates = sorted(df['date_only'].unique())\n",
        "    \n",
        "    # Determine which dates to analyze\n",
        "    if start_date is None:\n",
        "        # Use most recent dates\n",
        "        if len(all_dates) < num_days:\n",
        "            print(f\"⚠️ Only {len(all_dates)} days available, using all available dates\")\n",
        "            dates_to_analyze = all_dates\n",
        "        else:\n",
        "            dates_to_analyze = all_dates[-num_days:]  # Get the most recent dates\n",
        "    else:\n",
        "        # Use specified start date\n",
        "        start_date_obj = pd.to_datetime(start_date).date()\n",
        "        \n",
        "        # Find the start date in available dates\n",
        "        try:\n",
        "            start_idx = all_dates.index(start_date_obj)\n",
        "        except ValueError:\n",
        "            print(f\"❌ Start date {start_date} not found in dataset\")\n",
        "            print(f\"Available date range: {all_dates[0]} to {all_dates[-1]}\")\n",
        "            return None\n",
        "        \n",
        "        # Get the specified number of days starting from start_date\n",
        "        end_idx = min(start_idx + num_days, len(all_dates))\n",
        "        dates_to_analyze = all_dates[start_idx:end_idx]\n",
        "        \n",
        "        if len(dates_to_analyze) < num_days:\n",
        "            print(f\"⚠️ Only {len(dates_to_analyze)} days available from {start_date}\")\n",
        "    \n",
        "    print(f\"📅 Analyzing period: {dates_to_analyze[0]} to {dates_to_analyze[-1]}\")\n",
        "    \n",
        "    print(f\"📊 Analyzing {stock_name} for {len(dates_to_analyze)} days\")\n",
        "    print(f\"🔍 Using AI classifier to identify tweets mentioning {stock_name}\")\n",
        "    \n",
        "    # Analyze each date\n",
        "    correlation_data = []\n",
        "    dates = []\n",
        "    daily_returns_data = []\n",
        "    \n",
        "    for date in dates_to_analyze:\n",
        "        # Get ALL tweets for this date (not filtered by stock)\n",
        "        day_data = df[df['date_only'] == date]\n",
        "        \n",
        "        if len(day_data) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Analyze sentiment for this day\n",
        "        sentiments = []\n",
        "        daily_returns = []\n",
        "        tweets_analyzed = 0\n",
        "        tweets_with_target_stock = 0\n",
        "        \n",
        "        for idx, row in day_data.iterrows():\n",
        "            tweet_text = row['Tweet']\n",
        "            daily_return = row['daily_return']\n",
        "            \n",
        "            try:\n",
        "                # Use AI classifier to detect stocks mentioned in tweet\n",
        "                result = analyze_financial_text(tweet_text)\n",
        "                detected_stocks = result['stocks']\n",
        "                sentiment = result['sentiment']\n",
        "                \n",
        "                # Only analyze tweets where the AI classifier detected our target stock\n",
        "                if stock_name in detected_stocks:\n",
        "                    tweets_with_target_stock += 1\n",
        "                    \n",
        "                    # Convert sentiment to numeric value\n",
        "                    if sentiment == 'positive':\n",
        "                        sentiment_value = 1\n",
        "                    elif sentiment == 'negative':\n",
        "                        sentiment_value = -1\n",
        "                    else:  # unsure\n",
        "                        sentiment_value = 0\n",
        "                    \n",
        "                    sentiments.append(sentiment_value)\n",
        "                    daily_returns.append(daily_return)\n",
        "                \n",
        "                tweets_analyzed += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        print(f\"   📅 {date}: Analyzed {tweets_analyzed} tweets, found {tweets_with_target_stock} mentioning {stock_name}\")\n",
        "        \n",
        "        if sentiments and daily_returns:\n",
        "            # Calculate correlation for this day\n",
        "            if len(sentiments) > 1:\n",
        "                correlation = np.corrcoef(sentiments, daily_returns)[0, 1]\n",
        "            else:\n",
        "                correlation = 0\n",
        "            \n",
        "            correlation_data.append(correlation)\n",
        "            dates.append(date)\n",
        "            \n",
        "            # Get daily return for this stock on this date\n",
        "            stock_data_for_date = df[(df['date_only'] == date) & (df['Stock Name'] == stock_name)]\n",
        "            if len(stock_data_for_date) > 0:\n",
        "                daily_returns_data.append(stock_data_for_date['daily_return'].iloc[0] * 100)  # Convert to percentage\n",
        "            else:\n",
        "                daily_returns_data.append(0)\n",
        "    \n",
        "    if not correlation_data:\n",
        "        print(f\"❌ No correlation data available for {stock_name}\")\n",
        "        print(f\"   This could mean:\")\n",
        "        print(f\"   - No tweets were detected mentioning {stock_name}\")\n",
        "        print(f\"   - No sentiment data was available\")\n",
        "        return None\n",
        "    \n",
        "    # Create the plot\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
        "    \n",
        "    # Convert dates to datetime for plotting\n",
        "    date_objects = [datetime.combine(d, datetime.min.time()) for d in dates]\n",
        "    \n",
        "    # Plot 1: Correlation over time\n",
        "    ax1.plot(date_objects, correlation_data, marker='o', linewidth=2, markersize=6, color='blue')\n",
        "    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='No Correlation')\n",
        "    ax1.set_title(f'{stock_name} Sentiment-Price Correlation Over Time (AI-Detected Mentions)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Correlation Coefficient', fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Format x-axis\n",
        "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    ax1.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
        "    \n",
        "    # Plot 2: Daily returns over time\n",
        "    ax2.bar(date_objects, daily_returns_data, alpha=0.7, color='green', label='Daily Return %')\n",
        "    ax2.axhline(y=0, color='red', linestyle='-', alpha=0.5)\n",
        "    ax2.set_title(f'{stock_name} Daily Returns', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Daily Return (%)', fontsize=12)\n",
        "    ax2.set_xlabel('Date', fontsize=12)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "    \n",
        "    # Format x-axis\n",
        "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    ax2.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
        "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate and display statistics\n",
        "    avg_correlation = np.mean(correlation_data)\n",
        "    max_correlation = np.max(correlation_data)\n",
        "    min_correlation = np.min(correlation_data)\n",
        "    \n",
        "    print(f\"\\n📊 {stock_name} Correlation Statistics:\")\n",
        "    print(f\"   📈 Average Correlation: {avg_correlation:.3f}\")\n",
        "    print(f\"   📊 Max Correlation: {max_correlation:.3f}\")\n",
        "    print(f\"   📉 Min Correlation: {min_correlation:.3f}\")\n",
        "    print(f\"   📅 Days Analyzed: {len(dates)}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    if avg_correlation > 0.3:\n",
        "        print(f\"   ✅ Strong positive correlation between sentiment and price\")\n",
        "    elif avg_correlation > 0.1:\n",
        "        print(f\"   ⚠️ Moderate positive correlation between sentiment and price\")\n",
        "    elif avg_correlation > -0.1:\n",
        "        print(f\"   ❓ Weak correlation between sentiment and price\")\n",
        "    elif avg_correlation > -0.3:\n",
        "        print(f\"   ⚠️ Moderate negative correlation between sentiment and price\")\n",
        "    else:\n",
        "        print(f\"   ❌ Strong negative correlation between sentiment and price\")\n",
        "    \n",
        "    return {\n",
        "        'stock': stock_name,\n",
        "        'dates': dates,\n",
        "        'correlations': correlation_data,\n",
        "        'daily_returns': daily_returns_data,\n",
        "        'avg_correlation': avg_correlation,\n",
        "        'max_correlation': max_correlation,\n",
        "        'min_correlation': min_correlation\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 TESLA CORRELATION ANALYSIS - MOST RECENT 10 DAYS\n",
            "============================================================\n",
            "⚠️ Only 9 days available from 2022-09-19\n",
            "📅 Analyzing period: 2022-09-19 to 2022-09-29\n",
            "📊 Analyzing TSLA for 9 days\n",
            "🔍 Using AI classifier to identify tweets mentioning TSLA\n",
            "   📅 2022-09-19: Analyzed 396 tweets, found 294 mentioning TSLA\n"
          ]
        }
      ],
      "source": [
        "# Example: Plot correlation for Tesla over 10 days (most recent)\n",
        "print(\"📊 TESLA CORRELATION ANALYSIS - MOST RECENT 10 DAYS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "tesla_results = plot_stock_correlation('TSLA',start_date='2022-09-19', num_days=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Plot correlation for Apple starting from specific date\n",
        "print(\"\\n📊 APPLE CORRELATION ANALYSIS - SPECIFIC DATE RANGE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Analyze Apple starting from a specific date\n",
        "apple_results = plot_stock_correlation('AAPL', start_date='2022-09-29', num_days=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Compare multiple stocks over same period\n",
        "print(\"\\n📊 MULTI-STOCK COMPARISON - SAME PERIOD\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Analyze multiple stocks over the same 7-day period\n",
        "stocks_to_analyze = ['TSLA', 'AAPL', 'MSFT', 'GOOG', 'AMZN']\n",
        "comparison_results = {}\n",
        "start_date = '2022-09-29'  # Same start date for all stocks\n",
        "\n",
        "for stock in stocks_to_analyze:\n",
        "    print(f\"\\n🔍 Analyzing {stock} from {start_date}...\")\n",
        "    results = plot_stock_correlation(stock, start_date=start_date, num_days=7)\n",
        "    if results:\n",
        "        comparison_results[stock] = results['avg_correlation']\n",
        "\n",
        "# Summary comparison\n",
        "if comparison_results:\n",
        "    print(f\"\\n📊 CORRELATION COMPARISON SUMMARY:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    sorted_stocks = sorted(comparison_results.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    for stock, correlation in sorted_stocks:\n",
        "        print(f\"   {stock}: {correlation:.3f}\")\n",
        "    \n",
        "    best_stock = sorted_stocks[0]\n",
        "    worst_stock = sorted_stocks[-1]\n",
        "    \n",
        "    print(f\"\\n🏆 Best correlation: {best_stock[0]} ({best_stock[1]:.3f})\")\n",
        "    print(f\"📉 Worst correlation: {worst_stock[0]} ({worst_stock[1]:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
