{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FinbertBackbone(nn.Module):\n",
        "    def __init__(self, modelName: str = \"ProsusAI/finbert\"):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(modelName)\n",
        "        self.hiddenSize = self.encoder.config.hidden_size  # 768 \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        cls = out.last_hidden_state[:, 0]  # [CLS] token\n",
        "        return cls  # [batch, hidden]\n",
        "    \n",
        "class BinaryHead(nn.Module):\n",
        "    def __init__(self, inFeatures: int, pDrop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(pDrop)\n",
        "        self.fc = nn.Linear(inFeatures, 1)  # single logit\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc(x).squeeze(-1)    # [batch]\n",
        "        return logits\n",
        "\n",
        "class FinbertBinaryClf(nn.Module):\n",
        "    def __init__(self, modelName: str = \"ProsusAI/finbert\", pDrop: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.backbone = FinbertBackbone(modelName)\n",
        "        self.head = BinaryHead(self.backbone.hiddenSize, pDrop)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        feats = self.backbone(input_ids, attention_mask)\n",
        "        logits = self.head(feats)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text for sentiment analysis\"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)      \n",
        "    text = re.sub(r\"@\\w+\", \"\", text)        \n",
        "    text = re.sub(r\"^user:\\s*\", \"\", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\"^user\\s*\", \"\", text, flags=re.IGNORECASE)  \n",
        "    text = re.sub(r\"[\\\"]+\", \"\", text)        \n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() \n",
        "    return text\n",
        "\n",
        "def lables_zero_one(y: int) -> int:\n",
        "    \"\"\"Convert sentiment labels from {-1, 1} to {0, 1}\"\"\"\n",
        "    return 1 if int(y) == 1 else 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Models and Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading stock classification model...\n",
            "Stock tickers: ['AAPL', 'AMD', 'AMZN', 'BA', 'COST', 'DIS', 'GOOG', 'KO', 'META', 'MSFT', 'NFLX', 'NIO', 'Other', 'PG', 'PYPL', 'TSLA']\n",
            "Loading sentiment analysis model...\n",
            "âœ“ Both models loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "print(\"Loading stock classification model...\")\n",
        "stock_model_path = './results/checkpoint-5017/checkpoint-5017'  # Fixed path to nested directory\n",
        "stock_tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base')\n",
        "stock_model = AutoModelForSequenceClassification.from_pretrained(stock_model_path)\n",
        "stock_model.to(device)\n",
        "stock_model.eval()\n",
        "\n",
        "stock_tickers = ['AAPL', 'AMD', 'AMZN', 'BA', 'COST', 'DIS', 'GOOG', 'KO', 'META', 'MSFT', 'NFLX', 'NIO', 'Other', 'PG', 'PYPL', 'TSLA']\n",
        "print(f\"Stock tickers: {stock_tickers}\")\n",
        "\n",
        "# Load sentiment analysis model\n",
        "print(\"Loading sentiment analysis model...\")\n",
        "sentiment_model = FinbertBinaryClf(\"ProsusAI/finbert\", pDrop=0.1)\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "checkpoint = torch.load('finbert_finetuned.pt', map_location=device)\n",
        "sentiment_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "sentiment_model.to(device)\n",
        "sentiment_model.eval()\n",
        "\n",
        "print(\"Both models loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_stocks(tweet_text, model, tokenizer, threshold=0.5):\n",
        "    \"\"\"Predict which stocks are mentioned in a tweet\"\"\"\n",
        "    \n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    inputs = tokenizer(tweet_text, return_tensors='pt', truncation=True, max_length=128)\n",
        "    \n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    outputs = model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits)[0]\n",
        "    \n",
        "    probs = probs.cpu()\n",
        "    predictions = (probs > threshold).int().numpy()\n",
        "    \n",
        "    predicted_stocks = [stock_tickers[i] for i, pred in enumerate(predictions) if pred == 1]\n",
        "    \n",
        "    return predicted_stocks, probs.numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_sentiment(text, model, tokenizer, max_len=128, threshold=0.5, confidence_threshold=0.7):\n",
        "    \"\"\"Predict sentiment for input text\"\"\"\n",
        "    \n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    text = clean_text(text)\n",
        "    enc = tokenizer(\n",
        "        text, \n",
        "        truncation=True, \n",
        "        padding=\"max_length\", \n",
        "        max_length=max_len, \n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    \n",
        "    logits = model(enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device))\n",
        "    prob = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "    \n",
        "    if prob >= threshold:\n",
        "        label = 1\n",
        "        confidence = prob\n",
        "    else:\n",
        "        label = 0\n",
        "        confidence = 1 - prob\n",
        "    \n",
        "    if confidence < confidence_threshold:\n",
        "        label = -1  # -1 represents \"unsure\"\n",
        "    \n",
        "    return label, prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_financial_text(text):\n",
        "\n",
        "    \n",
        "    predicted_stocks, stock_probs = predict_stocks(\n",
        "        text, stock_model, stock_tokenizer, threshold=0.7\n",
        "    )\n",
        "    \n",
        "    sentiment_label, sentiment_prob = predict_sentiment(\n",
        "        text, sentiment_model, sentiment_tokenizer, threshold=0.5, confidence_threshold=0.7\n",
        "    )\n",
        "    \n",
        "    # Convert sentiment to readable format\n",
        "    if sentiment_label == 1:\n",
        "        sentiment = 'positive'\n",
        "        sentiment_confidence = sentiment_prob\n",
        "    elif sentiment_label == 0:\n",
        "        sentiment = 'negative'\n",
        "        sentiment_confidence = 1 - sentiment_prob\n",
        "    else:  \n",
        "        sentiment = 'unsure'\n",
        "        sentiment_confidence = max(sentiment_prob, 1 - sentiment_prob)\n",
        "    \n",
        "    stock_probabilities = {ticker: float(prob) for ticker, prob in zip(stock_tickers, stock_probs)}\n",
        "    \n",
        "    return {\n",
        "        'stocks': predicted_stocks,\n",
        "        'sentiment': sentiment,\n",
        "        'sentiment_confidence': float(sentiment_confidence),\n",
        "        'stock_probabilities': stock_probabilities\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FINANCIAL TEXT ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Text: Tesla and Apple crushing it! $TSLA $AAPL ðŸš€\n",
            "Predicted Stocks: ['AAPL']\n",
            "Sentiment: UNSURE (confidence: 0.661)\n",
            "Top Stock Probabilities: ['AAPL', 'TSLA']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Text: Microsoft stock is plummeting, terrible earnings report\n",
            "Predicted Stocks: []\n",
            "Sentiment: NEGATIVE (confidence: 0.920)\n",
            "Top Stock Probabilities: ['Unsure']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 3:\n",
            "Text: Amazon and Google both showing strong growth this quarter\n",
            "Predicted Stocks: ['AMZN']\n",
            "Sentiment: POSITIVE (confidence: 0.940)\n",
            "Top Stock Probabilities: ['AMZN']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 4:\n",
            "Text: Netflix subscription numbers are declining rapidly\n",
            "Predicted Stocks: ['NFLX']\n",
            "Sentiment: NEGATIVE (confidence: 0.914)\n",
            "Top Stock Probabilities: ['NFLX']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 5:\n",
            "Text: Meta's new VR headset is revolutionary! $META\n",
            "Predicted Stocks: ['META']\n",
            "Sentiment: POSITIVE (confidence: 0.940)\n",
            "Top Stock Probabilities: ['META']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 6:\n",
            "Text: Disney's streaming service is struggling with competition\n",
            "Predicted Stocks: ['AAPL']\n",
            "Sentiment: NEGATIVE (confidence: 0.892)\n",
            "Top Stock Probabilities: ['AAPL']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 7:\n",
            "Text: NVIDIA and AMD GPUs are in high demand for AI workloads\n",
            "Predicted Stocks: ['AMD']\n",
            "Sentiment: POSITIVE (confidence: 0.922)\n",
            "Top Stock Probabilities: ['AMD']\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 8:\n",
            "Text: PayPal's new features are disappointing users\n",
            "Predicted Stocks: []\n",
            "Sentiment: NEGATIVE (confidence: 0.914)\n",
            "Top Stock Probabilities: ['Unsure']\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test examples\n",
        "test_texts = [\n",
        "    \"Tesla and Apple crushing it! $TSLA $AAPL ðŸš€\",\n",
        "    \"Microsoft stock is plummeting, terrible earnings report\",\n",
        "    \"Amazon and Google both showing strong growth this quarter\",\n",
        "    \"Netflix subscription numbers are declining rapidly\",\n",
        "    \"Meta's new VR headset is revolutionary! $META\",\n",
        "    \"Disney's streaming service is struggling with competition\",\n",
        "    \"NVIDIA and AMD GPUs are in high demand for AI workloads\",\n",
        "    \"PayPal's new features are disappointing users\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"FINANCIAL TEXT ANALYSIS RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    print(f\"Text: {text}\")\n",
        "    \n",
        "    result = analyze_financial_text(text)\n",
        "    \n",
        "    print(f\"Predicted Stocks: {result['stocks']}\")\n",
        "    print(f\"Sentiment: {result['sentiment'].upper()} (confidence: {result['sentiment_confidence']:.3f})\")\n",
        "    \n",
        "    # Show top stock probabilities\n",
        "    top_stocks = [stock for stock, prob in result['stock_probabilities'].items() if prob > 0.5]\n",
        "    if top_stocks == []:\n",
        "        top_stocks = [\"Unsure\"]\n",
        "    #sorted(result['stock_probabilities'].items(), \n",
        "                      #key=lambda x: x[1], reverse=True)[:3]\n",
        "    print(f\"Top Stock Probabilities: {top_stocks}\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interactive_analysis():\n",
        "\n",
        "    while True:\n",
        "        text = input(\"\\nEnter text: \").strip()\n",
        "        \n",
        "        if text.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "            \n",
        "        if not text:\n",
        "            print(\"Please enter some text.\")\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            result = analyze_financial_text(text)\n",
        "            \n",
        "            print(f\"\\nANALYSIS RESULTS:\")\n",
        "            print(f\"Stocks: {', '.join(result['stocks']) if result['stocks'] else 'None detected'}\")\n",
        "            print(f\"Sentiment: {result['sentiment'].upper()} ({result['sentiment_confidence']:.1%} confidence)\")\n",
        "            \n",
        "            top_stocks = sorted(result['stock_probabilities'].items(), \n",
        "                              key=lambda x: x[1], reverse=True)[:3]\n",
        "            print(f\"Top Stock Probabilities:\")\n",
        "            for stock, prob in top_stocks:\n",
        "                print(f\"   {stock}: {prob:.1%}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing text: {e}\")\n",
        "\n",
        "# interactive_analysis()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "BATCH ANALYSIS EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Text 1: Apple's new iPhone sales are breaking records!\n",
            "  Stocks: ['AAPL']\n",
            "  Sentiment: positive (95.4%)\n",
            "\n",
            "Text 2: Tesla stock is crashing after the recall news\n",
            "  Stocks: ['TSLA']\n",
            "  Sentiment: negative (88.9%)\n",
            "\n",
            "Text 3: Microsoft Azure cloud revenue is growing rapidly\n",
            "  Stocks: []\n",
            "  Sentiment: positive (93.6%)\n"
          ]
        }
      ],
      "source": [
        "def batch_analyze(texts):\n",
        "\n",
        "    results = []\n",
        "    \n",
        "    for text in texts:\n",
        "        result = analyze_financial_text(text)\n",
        "        results.append({\n",
        "            'text': text,\n",
        "            'stocks': result['stocks'],\n",
        "            'sentiment': result['sentiment'],\n",
        "            'sentiment_confidence': result['sentiment_confidence']\n",
        "        })\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BATCH ANALYSIS EXAMPLE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "batch_texts = [\n",
        "    \"Apple's new iPhone sales are breaking records!\",\n",
        "    \"Tesla stock is crashing after the recall news\",\n",
        "    \"Microsoft Azure cloud revenue is growing rapidly\"\n",
        "]\n",
        "\n",
        "batch_results = batch_analyze(batch_texts)\n",
        "\n",
        "for i, result in enumerate(batch_results, 1):\n",
        "    print(f\"\\nText {i}: {result['text']}\")\n",
        "    print(f\"  Stocks: {result['stocks']}\")\n",
        "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_confidence']:.1%})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def analyze_sentiment_stock_correlation(target_date, combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Load combined dataset\n",
        "    print(\"Loading combined dataset\")\n",
        "    df = pd.read_csv(combined_data_path)\n",
        "    \n",
        "    df['date_only'] = pd.to_datetime(df['date_only']).dt.date\n",
        "    target_date_obj = pd.to_datetime(target_date).date()\n",
        "    \n",
        "    day_data = df[df['date_only'] == target_date_obj]\n",
        "    \n",
        "    print(f\"Found {len(day_data)} tweet-stock records for {target_date}\")\n",
        "    \n",
        "    if len(day_data) == 0:\n",
        "        return {\"error\": f\"No data found for {target_date}\"}\n",
        "    \n",
        "    print(\" Analyzing tweets with AI models...\")\n",
        "    \n",
        "    stock_analysis = defaultdict(lambda: {\n",
        "        'tweets': [],\n",
        "        'sentiments': [],\n",
        "        'sentiment_confidences': [],\n",
        "        'positive_count': 0,\n",
        "        'negative_count': 0,\n",
        "        'unsure_count': 0,\n",
        "        'total_tweets': 0,\n",
        "        'daily_return': None,\n",
        "        'price_data': None\n",
        "    })\n",
        "    \n",
        "    for idx, row in day_data.iterrows():\n",
        "        tweet_text = row['Tweet']\n",
        "        stock_name = row['Stock Name']\n",
        "        daily_return = row['daily_return']\n",
        "        \n",
        "        # Get price data for this stock\n",
        "        price_data = {\n",
        "            'open': row['Open'],\n",
        "            'close': row['Close'],\n",
        "            'high': row['High'],\n",
        "            'low': row['Low'],\n",
        "            'volume': row['Volume'],\n",
        "            'daily_return': daily_return\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            result = analyze_financial_text(tweet_text)\n",
        "            \n",
        "            # Store tweet info\n",
        "            tweet_info = {\n",
        "                'text': tweet_text,\n",
        "                'sentiment': result['sentiment'],\n",
        "                'confidence': result['sentiment_confidence'],\n",
        "                'mentioned_stocks': result['stocks']\n",
        "            }\n",
        "            \n",
        "            # Update analysis for this specific stock\n",
        "            if stock_name in result['stocks'] or stock_name in [s for s in result['stocks']]:\n",
        "                stock_analysis[stock_name]['tweets'].append(tweet_info)\n",
        "                stock_analysis[stock_name]['sentiments'].append(result['sentiment'])\n",
        "                stock_analysis[stock_name]['sentiment_confidences'].append(result['sentiment_confidence'])\n",
        "                stock_analysis[stock_name]['total_tweets'] += 1\n",
        "                stock_analysis[stock_name]['daily_return'] = daily_return\n",
        "                stock_analysis[stock_name]['price_data'] = price_data\n",
        "                \n",
        "                # Count sentiment types\n",
        "                if result['sentiment'] == 'positive':\n",
        "                    stock_analysis[stock_name]['positive_count'] += 1\n",
        "                elif result['sentiment'] == 'negative':\n",
        "                    stock_analysis[stock_name]['negative_count'] += 1\n",
        "                else:  # unsure\n",
        "                    stock_analysis[stock_name]['unsure_count'] += 1\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing tweet: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(\"\\nSENTIMENT ANALYSIS RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    results = {\n",
        "        'date': target_date,\n",
        "        'total_records_analyzed': len(day_data),\n",
        "        'stocks_analyzed': {},\n",
        "        'price_correlations': {}\n",
        "    }\n",
        "    \n",
        "    for stock, analysis in stock_analysis.items():\n",
        "        if analysis['total_tweets'] == 0:\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n {stock} Analysis:\")\n",
        "        print(f\"    Tweets mentioning {stock}: {analysis['total_tweets']}\")\n",
        "        print(f\"    Positive: {analysis['positive_count']} ({analysis['positive_count']/analysis['total_tweets']*100:.1f}%)\")\n",
        "        print(f\"    Negative: {analysis['negative_count']} ({analysis['negative_count']/analysis['total_tweets']*100:.1f}%)\")\n",
        "        print(f\"    Unsure: {analysis['unsure_count']} ({analysis['unsure_count']/analysis['total_tweets']*100:.1f}%)\")\n",
        "        \n",
        "        # Calculate overall sentiment\n",
        "        if analysis['positive_count'] > analysis['negative_count']:\n",
        "            overall_sentiment = 'positive'\n",
        "            sentiment_strength = analysis['positive_count'] / analysis['total_tweets']\n",
        "        elif analysis['negative_count'] > analysis['positive_count']:\n",
        "            overall_sentiment = 'negative'\n",
        "            sentiment_strength = analysis['negative_count'] / analysis['total_tweets']\n",
        "        else:\n",
        "            overall_sentiment = 'neutral'\n",
        "            sentiment_strength = 0.5\n",
        "        \n",
        "        print(f\"   Overall Sentiment: {overall_sentiment.upper()} (strength: {sentiment_strength:.2f})\")\n",
        "        \n",
        "        # Get stock price data\n",
        "        if analysis['price_data'] is not None:\n",
        "            price_data = analysis['price_data']\n",
        "            daily_return = analysis['daily_return']\n",
        "            daily_return_pct = daily_return * 100\n",
        "            \n",
        "            print(f\"    Stock Price: ${price_data['open']:.2f} â†’ ${price_data['close']:.2f}\")\n",
        "            print(f\"    Daily Return: {daily_return_pct:+.2f}%\")\n",
        "            \n",
        "            if daily_return > 0:\n",
        "                price_direction = 'up'\n",
        "            elif daily_return < 0:\n",
        "                price_direction = 'down'\n",
        "            else:\n",
        "                price_direction = 'flat'\n",
        "            \n",
        "            sentiment_price_match = (\n",
        "                (overall_sentiment == 'positive' and price_direction == 'up') or\n",
        "                (overall_sentiment == 'negative' and price_direction == 'down') or\n",
        "                (overall_sentiment == 'neutral' and price_direction == 'flat')\n",
        "            )\n",
        "            \n",
        "            correlation_status = \" MATCH\" if sentiment_price_match else \"NO MATCH\"\n",
        "            print(f\"   Correlation: {correlation_status}\")\n",
        "            \n",
        "            # Store results\n",
        "            results['stocks_analyzed'][stock] = {\n",
        "                'tweet_count': analysis['total_tweets'],\n",
        "                'sentiment_breakdown': {\n",
        "                    'positive': analysis['positive_count'],\n",
        "                    'negative': analysis['negative_count'],\n",
        "                    'unsure': analysis['unsure_count']\n",
        "                },\n",
        "                'overall_sentiment': overall_sentiment,\n",
        "                'sentiment_strength': sentiment_strength,\n",
        "                'price_data': {\n",
        "                    'open': float(price_data['open']),\n",
        "                    'close': float(price_data['close']),\n",
        "                    'high': float(price_data['high']),\n",
        "                    'low': float(price_data['low']),\n",
        "                    'volume': int(price_data['volume']),\n",
        "                    'daily_return': float(daily_return),\n",
        "                    'daily_return_pct': float(daily_return_pct),\n",
        "                    'direction': price_direction\n",
        "                },\n",
        "                'correlation_match': sentiment_price_match\n",
        "            }\n",
        "            \n",
        "            results['price_correlations'][stock] = {\n",
        "                'sentiment': overall_sentiment,\n",
        "                'price_direction': price_direction,\n",
        "                'match': sentiment_price_match\n",
        "            }\n",
        "        else:\n",
        "            print(f\"   No price data found for {stock}\")\n",
        "            results['stocks_analyzed'][stock] = {\n",
        "                'tweet_count': analysis['total_tweets'],\n",
        "                'sentiment_breakdown': {\n",
        "                    'positive': analysis['positive_count'],\n",
        "                    'negative': analysis['negative_count'],\n",
        "                    'unsure': analysis['unsure_count']\n",
        "                },\n",
        "                'overall_sentiment': overall_sentiment,\n",
        "                'sentiment_strength': sentiment_strength,\n",
        "                'price_data': None,\n",
        "                'correlation_match': None\n",
        "            }\n",
        "    \n",
        "    print(f\"\\nCORRELATION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    total_stocks = len([s for s in results['stocks_analyzed'].keys() if results['stocks_analyzed'][s]['price_data'] is not None])\n",
        "    matches = sum(1 for s in results['price_correlations'].values() if s['match'])\n",
        "    \n",
        "    if total_stocks > 0:\n",
        "        correlation_rate = matches / total_stocks * 100\n",
        "        print(f\" Overall Correlation Rate: {correlation_rate:.1f}% ({matches}/{total_stocks} stocks)\")\n",
        "        \n",
        "        if correlation_rate >= 70:\n",
        "            print(\" Strong correlation between sentiment and stock prices!\")\n",
        "        elif correlation_rate >= 50:\n",
        "            print(\"Moderate correlation between sentiment and stock prices\")\n",
        "        else:\n",
        "            print(\"Weak correlation between sentiment and stock prices\")\n",
        "    else:\n",
        "        print(\"No stock price data available for correlation analysis\")\n",
        "    \n",
        "    results['correlation_summary'] = {\n",
        "        'total_stocks_with_prices': total_stocks,\n",
        "        'matches': matches,\n",
        "        'correlation_rate': correlation_rate if total_stocks > 0 else 0\n",
        "    }\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_multiple_dates(date_list, combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "\n",
        "    \n",
        "    print(f\" Analyzing {len(date_list)} dates for sentiment-stock correlation\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    all_results = {}\n",
        "    total_correlations = []\n",
        "    \n",
        "    for date in date_list:\n",
        "        print(f\"\\n Analyzing {date}.\")\n",
        "        result = analyze_sentiment_stock_correlation(date, combined_data_path)\n",
        "        all_results[date] = result\n",
        "        \n",
        "        if 'correlation_summary' in result:\n",
        "            total_correlations.append(result['correlation_summary']['correlation_rate'])\n",
        "    \n",
        "    # Calculate overall statistics\n",
        "    if total_correlations:\n",
        "        avg_correlation = np.mean(total_correlations)\n",
        "        print(f\"\\nOVERALL ANALYSIS SUMMARY\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Dates analyzed: {len(date_list)}\")\n",
        "        print(f\"Average correlation rate: {avg_correlation:.1f}%\")\n",
        "        print(f\"Correlation range: {min(total_correlations):.1f}% - {max(total_correlations):.1f}%\")\n",
        "        \n",
        "        if avg_correlation >= 70:\n",
        "            print(\"Strong overall correlation between sentiment and stock prices!\")\n",
        "        elif avg_correlation >= 50:\n",
        "            print(\"Moderate overall correlation between sentiment and stock prices\")\n",
        "        else:\n",
        "            print(\"Weak overall correlation between sentiment and stock prices\")\n",
        "    \n",
        "    return all_results\n",
        "\n",
        "def get_available_dates(combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "\n",
        "    \n",
        "    df = pd.read_csv(combined_data_path)\n",
        "    df['date_only'] = pd.to_datetime(df['date_only']).dt.date\n",
        "    \n",
        "    available_dates = sorted(df['date_only'].unique())\n",
        "    \n",
        "    # Get some statistics\n",
        "    total_records = len(df)\n",
        "    unique_stocks = df['Stock Name'].nunique()\n",
        "    \n",
        "    return {\n",
        "        'available_dates': [d.strftime('%Y-%m-%d') for d in available_dates],\n",
        "        'total_dates': len(available_dates),\n",
        "        'total_records': total_records,\n",
        "        'unique_stocks': unique_stocks,\n",
        "        'date_range': {\n",
        "            'start': available_dates[0].strftime('%Y-%m-%d'),\n",
        "            'end': available_dates[-1].strftime('%Y-%m-%d')\n",
        "        }\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check available dates in the combined dataset\n",
        "print(\" CHECKING AVAILABLE DATES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "available_dates = get_available_dates()\n",
        "print(f\" Total dates: {available_dates['total_dates']}\")\n",
        "print(f\"Total records: {available_dates['total_records']:,}\")\n",
        "print(f\"Unique stocks: {available_dates['unique_stocks']}\")\n",
        "print(f\"Date range: {available_dates['date_range']['start']} to {available_dates['date_range']['end']}\")\n",
        "\n",
        "# Show some example dates\n",
        "if available_dates['available_dates']:\n",
        "    print(f\"\\nExample available dates:\")\n",
        "    for date in available_dates['available_dates'][:5]:\n",
        "        print(f\"   {date}\")\n",
        "    if len(available_dates['available_dates']) > 5:\n",
        "        print(f\"   ... and {len(available_dates['available_dates']) - 5} more dates\")\n",
        "else:\n",
        "    print(\"No dates found in the combined dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Analyze a specific date\n",
        "if available_dates['available_dates']:\n",
        "    # Use the first available date as an example\n",
        "    example_date = available_dates['available_dates'][0]\n",
        "    print(f\"\\nEXAMPLE ANALYSIS FOR {example_date}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Run the analysis\n",
        "    result = analyze_sentiment_stock_correlation(example_date)\n",
        "    \n",
        "    if 'error' not in result:\n",
        "        print(f\"\\nSUMMARY FOR {example_date}:\")\n",
        "        print(f\" Total records analyzed: {result['total_records_analyzed']}\")\n",
        "        print(f\"Stocks analyzed: {len(result['stocks_analyzed'])}\")\n",
        "        if 'correlation_summary' in result:\n",
        "            print(f\"Correlation rate: {result['correlation_summary']['correlation_rate']:.1f}%\")\n",
        "    else:\n",
        "        print(f\"Error: {result['error']}\")\n",
        "else:\n",
        "    print(\"No dates available for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if available_dates['available_dates'] and len(available_dates['available_dates']) >= 3:\n",
        "    print(f\"\\nMULTIPLE DATE ANALYSIS EXAMPLE\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    dates_to_analyze = available_dates['available_dates'][:3]\n",
        "    print(f\"Analyzing dates: {dates_to_analyze}\")\n",
        "    \n",
        "    multi_results = analyze_multiple_dates(dates_to_analyze)\n",
        "    \n",
        "    print(f\"\\nMULTI-DATE SUMMARY:\")\n",
        "    for date, result in multi_results.items():\n",
        "        if 'correlation_summary' in result:\n",
        "            print(f\"   {date}: {result['correlation_summary']['correlation_rate']:.1f}% correlation\")\n",
        "        else:\n",
        "            print(f\"   {date}: No data available\")\n",
        "else:\n",
        "    print(\"Not enough dates available for multi-date analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def plot_stock_correlation(stock_name, start_date=None, num_days=10, combined_data_path='data/processed/filtered_tweets_with_stock_data.csv'):\n",
        "\n",
        "    \n",
        "    df = pd.read_csv(combined_data_path)\n",
        "    df['date_only'] = pd.to_datetime(df['date_only']).dt.date\n",
        "    \n",
        "    all_dates = sorted(df['date_only'].unique())\n",
        "    \n",
        "    # Determine which dates to analyze\n",
        "    if start_date is None:\n",
        "        # Use most recent dates\n",
        "        if len(all_dates) < num_days:\n",
        "            print(f\" Only {len(all_dates)} days available, using all available dates\")\n",
        "            dates_to_analyze = all_dates\n",
        "        else:\n",
        "            dates_to_analyze = all_dates[-num_days:]  # Get the most recent dates\n",
        "    else:\n",
        "        # Use specified start date\n",
        "        start_date_obj = pd.to_datetime(start_date).date()\n",
        "        \n",
        "        # Find the start date in available dates\n",
        "        try:\n",
        "            start_idx = all_dates.index(start_date_obj)\n",
        "        except ValueError:\n",
        "            print(f\" Start date {start_date} not found in dataset\")\n",
        "            print(f\"Available date range: {all_dates[0]} to {all_dates[-1]}\")\n",
        "            return None\n",
        "        \n",
        "        # Get the specified number of days starting from start_date\n",
        "        end_idx = min(start_idx + num_days, len(all_dates))\n",
        "        dates_to_analyze = all_dates[start_idx:end_idx]\n",
        "        \n",
        "        if len(dates_to_analyze) < num_days:\n",
        "            print(f\" Only {len(dates_to_analyze)} days available from {start_date}\")\n",
        "    \n",
        "    print(f\"Analyzing period: {dates_to_analyze[0]} to {dates_to_analyze[-1]}\")\n",
        "    \n",
        "    print(f\"Analyzing {stock_name} for {len(dates_to_analyze)} days\")\n",
        "    print(f\"Using AI classifier to identify tweets mentioning {stock_name}\")\n",
        "    \n",
        "    # Analyze each date\n",
        "    correlation_data = []\n",
        "    dates = []\n",
        "    daily_returns_data = []\n",
        "    daily_sentiment_data = []\n",
        "    \n",
        "    for date in dates_to_analyze:\n",
        "        day_data = df[df['date_only'] == date]\n",
        "        \n",
        "        if len(day_data) == 0:\n",
        "            continue\n",
        "        \n",
        "        sentiments = []\n",
        "        daily_returns = []\n",
        "        tweets_analyzed = 0\n",
        "        tweets_with_target_stock = 0\n",
        "        \n",
        "        for idx, row in day_data.iterrows():\n",
        "            tweet_text = row['Tweet']\n",
        "            daily_return = row['daily_return']\n",
        "            \n",
        "            try:\n",
        "                predicted_stocks, stock_probs = predict_stocks(tweet_text, stock_model, stock_tokenizer, threshold=0.5)\n",
        "                \n",
        "                if stock_name in predicted_stocks:\n",
        "                    tweets_with_target_stock += 1\n",
        "                    \n",
        "                    sentiment_label, sentiment_prob = predict_sentiment(\n",
        "                        tweet_text, sentiment_model, sentiment_tokenizer, \n",
        "                        threshold=0.5, confidence_threshold=0.7\n",
        "                    )\n",
        "                    \n",
        "                    if sentiment_label == 1:\n",
        "                        sentiment_value = 1\n",
        "                    elif sentiment_label == 0:\n",
        "                        sentiment_value = -1\n",
        "                    else:  # unsure (sentiment_label == -1)\n",
        "                        sentiment_value = 0\n",
        "                    \n",
        "                    sentiments.append(sentiment_value)\n",
        "                    daily_returns.append(daily_return)\n",
        "                \n",
        "                tweets_analyzed += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                continue\n",
        "        \n",
        "        print(f\"{date}: Analyzed {tweets_analyzed} tweets, found {tweets_with_target_stock} mentioning {stock_name}\")\n",
        "        \n",
        "        if sentiments and daily_returns:\n",
        "            # Calculate correlation for this day\n",
        "            if len(sentiments) > 1:\n",
        "                correlation = np.corrcoef(sentiments, daily_returns)[0, 1]\n",
        "            else:\n",
        "                correlation = 0\n",
        "            \n",
        "            # Calculate average daily sentiment\n",
        "            avg_sentiment = np.mean(sentiments)\n",
        "            \n",
        "            correlation_data.append(correlation)\n",
        "            dates.append(date)\n",
        "            daily_sentiment_data.append(avg_sentiment)\n",
        "            \n",
        "            # Get daily return for this stock on this date\n",
        "            stock_data_for_date = df[(df['date_only'] == date) & (df['Stock Name'] == stock_name)]\n",
        "            if len(stock_data_for_date) > 0:\n",
        "                daily_returns_data.append(stock_data_for_date['daily_return'].iloc[0] * 100)  # Convert to percentage\n",
        "            else:\n",
        "                daily_returns_data.append(0)\n",
        "    \n",
        "    if not correlation_data:\n",
        "        print(f\"No correlation data available for {stock_name}\")\n",
        "        print(f\" This could mean:\")\n",
        "        print(f\" - No tweets were detected mentioning {stock_name}\")\n",
        "        print(f\" - No sentiment data was available\")\n",
        "        return None\n",
        "    \n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
        "    \n",
        "    date_objects = [datetime.combine(d, datetime.min.time()) for d in dates]\n",
        "    \n",
        "    ax1.plot(date_objects, correlation_data, marker='o', linewidth=2, markersize=6, color='blue')\n",
        "    ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='No Correlation')\n",
        "    ax1.set_title(f'{stock_name} Sentiment-Price Correlation Over Time (AI-Detected Mentions)', fontsize=14, fontweight='bold')\n",
        "    ax1.set_ylabel('Correlation Coefficient', fontsize=12)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Format x-axis\n",
        "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    ax1.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
        "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
        "    \n",
        "    ax2.plot(date_objects, daily_sentiment_data, marker='o', linewidth=2, markersize=6, color='purple', label='Daily Sentiment')\n",
        "    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Neutral Sentiment')\n",
        "    ax2.axhline(y=0.5, color='green', linestyle=':', alpha=0.7, label='Positive Threshold')\n",
        "    ax2.axhline(y=-0.5, color='orange', linestyle=':', alpha=0.7, label='Negative Threshold')\n",
        "    ax2.set_title(f'{stock_name} Daily Sentiment Over Time', fontsize=14, fontweight='bold')\n",
        "    ax2.set_ylabel('Average Sentiment', fontsize=12)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "    ax2.set_ylim(-1.1, 1.1)\n",
        "    \n",
        "    # Format x-axis\n",
        "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    ax2.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
        "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
        "    \n",
        "    ax3.bar(date_objects, daily_returns_data, alpha=0.7, color='green', label='Daily Return %')\n",
        "    ax3.axhline(y=0, color='red', linestyle='-', alpha=0.5)\n",
        "    ax3.set_title(f'{stock_name} Daily Returns', fontsize=14, fontweight='bold')\n",
        "    ax3.set_ylabel('Daily Return (%)', fontsize=12)\n",
        "    ax3.set_xlabel('Date', fontsize=12)\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    ax3.legend()\n",
        "    \n",
        "    ax3.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "    ax3.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
        "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    import os\n",
        "    os.makedirs('correlation_plots', exist_ok=True)\n",
        "    \n",
        "    start_str = dates[0].strftime('%Y%m%d')\n",
        "    end_str = dates[-1].strftime('%Y%m%d')\n",
        "    filename = f'correlation_plots/{stock_name}_correlation_{start_str}_to_{end_str}.png'\n",
        "    \n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Plot saved as: {filename}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate and display statistics\n",
        "    avg_correlation = np.mean(correlation_data)\n",
        "    max_correlation = np.max(correlation_data)\n",
        "    min_correlation = np.min(correlation_data)\n",
        "    \n",
        "    print(f\"\\n{stock_name} Correlation Statistics:\")\n",
        "    print(f\" Average Correlation: {avg_correlation:.3f}\")\n",
        "    print(f\" Max Correlation: {max_correlation:.3f}\")\n",
        "    print(f\" Min Correlation: {min_correlation:.3f}\")\n",
        "    print(f\" Days Analyzed: {len(dates)}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    if avg_correlation > 0.3:\n",
        "        print(f\"Strong positive correlation between sentiment and price\")\n",
        "    elif avg_correlation > 0.1:\n",
        "        print(f\" Moderate positive correlation between sentiment and price\")\n",
        "    elif avg_correlation > -0.1:\n",
        "        print(f\"  Weak correlation between sentiment and price\")\n",
        "    elif avg_correlation > -0.3:\n",
        "        print(f\"Moderate negative correlation between sentiment and price\")\n",
        "    else:\n",
        "        print(f\"Strong negative correlation between sentiment and price\")\n",
        "    \n",
        "    return {\n",
        "        'stock': stock_name,\n",
        "        'dates': dates,\n",
        "        'correlations': correlation_data,\n",
        "        'daily_returns': daily_returns_data,\n",
        "        'daily_sentiment': daily_sentiment_data,\n",
        "        'avg_correlation': avg_correlation,\n",
        "        'max_correlation': max_correlation,\n",
        "        'min_correlation': min_correlation\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tesla_results = plot_stock_correlation('TSLA',start_date='2022-09-19', num_days=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nAPPLE CORRELATION ANALYSIS - SPECIFIC DATE RANGE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "apple_results = plot_stock_correlation('AAPL', start_date='2022-09-29', num_days=7)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Compare multiple stocks over same period\n",
        "print(\"\\nMULTI-STOCK COMPARISON - SAME PERIOD\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "stocks_to_analyze = ['TSLA', 'AAPL', 'MSFT', 'GOOG', 'AMZN']\n",
        "comparison_results = {}\n",
        "start_date = '2022-09-29'  # Same start date for all stocks\n",
        "\n",
        "for stock in stocks_to_analyze:\n",
        "    print(f\"\\nAnalyzing {stock} from {start_date}...\")\n",
        "    results = plot_stock_correlation(stock, start_date=start_date, num_days=7)\n",
        "    if results:\n",
        "        comparison_results[stock] = results['avg_correlation']\n",
        "\n",
        "if comparison_results:\n",
        "    print(f\"\\nCORRELATION COMPARISON SUMMARY:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    sorted_stocks = sorted(comparison_results.items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    for stock, correlation in sorted_stocks:\n",
        "        print(f\"   {stock}: {correlation:.3f}\")\n",
        "    \n",
        "    best_stock = sorted_stocks[0]\n",
        "    worst_stock = sorted_stocks[-1]\n",
        "    \n",
        "    print(f\"\\n Best correlation: {best_stock[0]} ({best_stock[1]:.3f})\")\n",
        "    print(f\"Worst correlation: {worst_stock[0]} ({worst_stock[1]:.3f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
